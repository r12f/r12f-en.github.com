<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:new URL("http://en.r12f.com").hostname,root:"/",scheme:"Pisces",version:"7.6.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"disqus",storage:!0,lazyload:!1,nav:null,activeClass:"disqus"},algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"",motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}}}</script><meta name="description" content="In the last post, we discussed the reasons for writing tests, principles and how to write code that is easy to test. In this post, we&#39;ll discuss more on how to write good tests.First, let me repea"><meta property="og:type" content="article"><meta property="og:title" content="Pragmatic testing (P2: Good tests, Metadata-based testing, Fake tests)"><meta property="og:url" content="http://en.r12f.com/posts/pragmatic-testing-part-2/index.html"><meta property="og:site_name" content="Soul Orbit"><meta property="og:description" content="In the last post, we discussed the reasons for writing tests, principles and how to write code that is easy to test. In this post, we&#39;ll discuss more on how to write good tests.First, let me repea"><meta property="og:locale" content="en_US"><meta property="article:published_time" content="2021-02-15T22:56:13.000Z"><meta property="article:modified_time" content="2021-02-17T04:15:41.358Z"><meta property="article:author" content="r12f"><meta property="article:tag" content="Testing"><meta property="article:tag" content="Coding"><meta property="article:tag" content="Software Engineering"><meta name="twitter:card" content="summary"><link rel="canonical" href="http://en.r12f.com/posts/pragmatic-testing-part-2/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0}</script><title>Pragmatic testing (P2: Good tests, Metadata-based testing, Fake tests) | Soul Orbit</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-20527248-4"></script><script>if(CONFIG.hostname===location.hostname){function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-20527248-4")}</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="Soul Orbit" type="application/atom+xml">
</head><body itemscope itemtype="http://schema.org/WebPage"><div class="container"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Soul Orbit</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">I'll take a quiet life. A handshake of carbon monoxide.</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="Toggle navigation bar"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>Sitemap</a></li></ul></nav></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en"><link itemprop="mainEntityOfPage" href="http://en.r12f.com/posts/pragmatic-testing-part-2/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="r12f"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Soul Orbit"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Pragmatic testing (P2: Good tests, Metadata-based testing, Fake tests)</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2021-02-15 14:56:13" itemprop="dateCreated datePublished" datetime="2021-02-15T14:56:13-08:00">2021-02-15</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/01-Binary-Life/" itemprop="url" rel="index"><span itemprop="name">01 Binary Life</span> </a></span></span><span id="/posts/pragmatic-testing-part-2/" class="post-meta-item leancloud_visitors" data-flag-title="Pragmatic testing (P2: Good tests, Metadata-based testing, Fake tests)" title="Views"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">Views: </span><span class="leancloud-visitors-count"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i> </span><span class="post-meta-item-text">Disqus: </span><a title="disqus" href="/posts/pragmatic-testing-part-2/#comments" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="posts/pragmatic-testing-part-2/" itemprop="commentCount"></span></a></span></div></header><div class="post-body" itemprop="articleBody"><p><a href="/posts/pragmatic-testing-part-1">In the last post, we discussed the reasons for writing tests, principles and how to write code that is easy to test</a>. In this post, we&#39;ll discuss more on how to write good tests.</p><p>First, let me repeat the important thing: <strong>Test is a not a silver bullet.</strong> Adding a few tests won&#39;t help us improve the code quality that much. If you haven&#39;t read the <a href="/posts/pragmatic-testing-part-1">part 1</a> yet, it is highly recommended to read it first.</p><hr><h2 id="Good-tests">1. Good tests</h2><p>We are finally here! Let&#39;s talk about writing tests! Making code easy to test is only the first step, and to write good tests we will need some more skills.</p><h3 id="Choose-the-right-way-to-test">1.1. Choose the right way to test</h3><p>We know that there are various types of tests. And here are some examples:</p><ul><li><strong>Unit test</strong>: Test a class or a small module by talking to them directly, e.g. calling public member functions.</li><li><strong>Module/Service test</strong>: Test a module or service by simulating its upstream and downstream services.</li><li><strong>End-To-End test (E2E test)</strong>: Test a complete system by simulating the upstream and downstream services of the whole system.</li><li><strong>Integration test</strong>: Integrate all services, even multiple systems, as required and test them.</li><li><strong>In-Service/Probe test</strong>: Test the services within the service itself or with a dedicated probe service at regular intervals.</li></ul><a id="more"></a><p>Each of these tests has its own advantages and disadvantages, roughly as follows.</p><table><thead><tr><th>Test type</th><th>Implementation difficulty</th><th>Local testing?</th><th>Test speed</th><th>Time to find problems</th><th>Complexity of finding problems</th><th>Difficulty of debugging problems</th></tr></thead><tbody><tr><td>Unit Testing</td><td>Easy</td><td>Yes</td><td>Fast</td><td>Early</td><td>Simple</td><td>Easy</td></tr><tr><td>Module/Service Testing</td><td>Relatively Easy</td><td>Doable</td><td>Relatively Fast</td><td>Relatively Early</td><td>Normal</td><td>Relatively Simple<br>(if local debugging is possible)</td></tr><tr><td>End-to-end testing</td><td>Normal</td><td>Doable</td><td>Nromal</td><td>Normal</td><td>Relatively Complex</td><td>Relatively Simple<br>(if local debugging is possible)</td></tr><tr><td>Integration Testing</td><td>Hard</td><td>Usually No</td><td>Slow</td><td>Late</td><td>Complex</td><td>Hard</td></tr><tr><td>Probe Testing</td><td>Normal</td><td>Usually No</td><td>Relatively Fast</td><td>Late</td><td>Normal</td><td>Relatively Hard</td></tr></tbody></table><p>Therefore, when writing tests, we need to think about what we want to test and choose the appropriate way to test it.</p><p>When choosing, we must pay attention to the <a href="/posts/pragmatic-testing-part-1Principles-of-testing">shortest distance principle</a>. For example, we may feel that the integration test will have the best coverage, so we should test everything with integration tests. But precisely because of its coverage, we may spend a tremendous amount of work building these tests, and debugging them when test fails. Imagine an exception is thrown in a very strange place after dozens of services have called each other hundreds of times. And then, we need to debug this and find out why... (Good luck with this :D)</p><hr><h3 id="Reasonable-prompt-messages">1.2. Reasonable prompt messages</h3><blockquote><p>&quot;Transparency is a passive quality. A program is transparent when it is possible to form a simple mental model of its behavior that is actually predictive for all or most cases, because you can see through the machinery to what is actually going on.&quot;</p><p>- Eric Steven Raymond, from &quot;The Art of Unix Programming&quot;</p></blockquote><p>While tests can tell us that something is wrong, many people often miss the point that: <strong>it is more important to tell us what is wrong and what to check</strong>.</p><h4 id="Clarify-the-test-scenarios">1.2.1. Clarify the test scenarios</h4><p>The first and most overlooked thing is the names of the tests. Here are some examples that I&#39;ve (often) seen in real projects:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Test</span><br><span class="line">Test1/<span class="number">2</span>/<span class="number">3</span>/...</span><br><span class="line">TestSwitch</span><br><span class="line">TestBasic</span><br><span class="line">MyTest</span><br><span class="line">FooTest/TestFoo <span class="comment">// Foo is the name of the class that being tested</span></span><br></pre></td></tr></table></figure><p>These test names won&#39;t help us understand their purpose at all, so when something goes wrong we have no idea what to look at. A better way here would be to <strong>describe the test scenario well and use assertive test name</strong>.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ScopeHandle_AfterDtor_UnderlyingHandleShouldBeClosed</span><br><span class="line">CrontabTimer_ParsingValidCrontabSyntax_ShouldSucceed</span><br><span class="line">DataProcessor_ProcessingDataWithUnexpectedFormat_ShouldFail</span><br><span class="line">HealthProber_ProbeUnresponsiveHttpTarget_ShouldFailWithTimeout</span><br></pre></td></tr></table></figure><p>Don&#39;t worry about long function names. Being descriptive is never wrong.</p><h4 id="Actionable-error-messages">1.2.2. Actionable error messages</h4><p>When test fails, the error message must be clear and actionable.</p><p>For example: when writing assertions, don&#39;t just simply write something like <code>Assert(a != 0);</code>. It is better to give some advice:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Assert(dataIterator != dataMap.<span class="built_in">end</span>(),</span><br><span class="line">       <span class="string">"Data is not created while id still exists. It might be a leak on id. Please check CreateData/RemoveData functions."</span>);</span><br></pre></td></tr></table></figure><p>Debugging such error will be much easier because the error is very clear and specific.</p><h4 id="Make-behavior-changes-obvious">1.2.3. Make behavior changes obvious</h4><p>As we mentioned before, one of the purpose of testing is to identify behavior changes. So, to ensure high observability, these behavior changes must be as clear and obvious as possible. These include changes in our service internal states, data reporting, and even behavior of debugging related tools (e.g., APIs for getting certain state of the service).</p><p>This idea is great, but it also leads to the problem - If we want to see all the behavior changes, we have to write code to test every behavior our program. This makes it extremely hard to maintain these tests!</p><p>Here&#39;s a simple example: <a href="https://www.oreilly.com/library/view/building-microservices/9781491950340/ch07.html#idp3960144" target="_blank" rel="noopener">Testing a microservice by simulating a service upstream and downstream</a> is a very common practise, so I&#39;m sure you&#39;ve seen countless of tests that look like this:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">TEST_METHOD(MyService_WhenReceivingValidRequest_DownstreamServicesShouldBeProgrammed)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;MyService&gt; service = CreateMyService();</span><br><span class="line"></span><br><span class="line">    MyServiceRequest mockRequest;</span><br><span class="line">    mockRequest.Foo = <span class="number">1</span>;</span><br><span class="line">    mockRequest.Bar = <span class="number">2</span>;</span><br><span class="line">    <span class="comment">// ... Update all fields here.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Simulating upstream service sending state to my service</span></span><br><span class="line">    service-&gt;SendRequestSync(mockRequest);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Test service internal states</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;<span class="keyword">const</span> MyServiceState&gt; serviceState = service-&gt;GetInternalState()-&gt;GetPayload&lt;MyServiceState&gt;();</span><br><span class="line">    Assert::AreEqual(<span class="number">1</span>, serviceState-&gt;Foo);</span><br><span class="line">    Assert::AreEqual(<span class="number">2</span>, serviceState-&gt;Bar);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Validate downstream services</span></span><br><span class="line">    ServiceDownstreamStates statesToDownstreamServices = service-&gt;GetStatesToDownStreamServices();</span><br><span class="line">    Assert::AreEqual(<span class="number">2u</span>ll, statesToDownstreamServices.<span class="built_in">size</span>());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Validate states send to downstream service 1</span></span><br><span class="line">    Assert::IsTrue(statesToDownstreamServices.<span class="built_in">find</span>(<span class="string">L"MyDownstreamService1"</span>) != statesToDownstreamServices.<span class="built_in">end</span>());</span><br><span class="line">    Assert::AreEqual(<span class="number">1u</span>ll, statesToDownstreamServices[<span class="string">L"MyDownstreamService1"</span>].States.<span class="built_in">size</span>());</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;<span class="keyword">const</span> MyDownstreamService1Request&gt; stateToDownstreamService1 = statesToDownstreamServices[<span class="string">L"MyDownstreamService1"</span>].States[<span class="number">0</span>].GetPayload&lt;MyDownstreamService1Request&gt;();</span><br><span class="line">    Assert::IsNotNull(stateToDownstreamService1);</span><br><span class="line">    Assert::AreEqual(<span class="number">1</span>, stateToDownstreamService1-&gt;Foo);</span><br><span class="line">    <span class="comment">// ... Validate all other states</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Validate states send to downstream service 2</span></span><br><span class="line">    Assert::IsTrue(statesToDownstreamServices.<span class="built_in">find</span>(<span class="string">L"MyDownstreamService2"</span>) != statesToDownstreamServices.<span class="built_in">end</span>());</span><br><span class="line">    Assert::AreEqual(<span class="number">1u</span>ll, statesToDownstreamServices[<span class="string">L"MyDownstreamService2"</span>].States.<span class="built_in">size</span>());</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;<span class="keyword">const</span> MyDownstreamService2Request&gt; stateToDownstreamService2 = statesToDownstreamServices[<span class="string">L"MyDownstreamService2"</span>].States[<span class="number">0</span>].GetPayload&lt;MyDownstreamService2Request&gt;();</span><br><span class="line">    Assert::IsNotNull(stateToDownstreamService2);</span><br><span class="line">    Assert::AreEqual(<span class="number">2</span>, stateToDownstreamService2-&gt;Bar);</span><br><span class="line">    <span class="comment">// ... Validate all other states</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Validate metrics</span></span><br><span class="line">    MyServiceTrackedStats trackedStats = service-&gt;GetStatsTracker()-&gt;GetTrackedStats();</span><br><span class="line">    Assert::AreEqual(<span class="number">1</span>, trackedStates.RequestCount);</span><br><span class="line">    Assert::AreEqual(<span class="number">1</span>, trackedStates.Service1RequestSent);</span><br><span class="line">    Assert::AreEqual(<span class="number">1</span>, trackedStates.Service2RequestSent);</span><br><span class="line">    <span class="comment">// ... Validate all other stats</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>As we can see, even with the help from a lot of helper functions to simplify the code, the tests are still very tedious to write. And once any behavior change is made, all the tests must be changed along with it. This leads to an extremely high workload in maintaining the tests, which makes everyone so tired of adding more. So, is there a way to have the best of both worlds? Gladly, yes! Later, we will introduce metadata-based testing to help us solve this problem.</p><h3 id="Focus-on-requirement-and-defect-coverage">1.3. Focus on requirement and defect coverage</h3><p>Nowadays, many testing tools can provide code coverage. And high code coverage is also enforced in many projects. While this indeed helps, it can also mislead people, making people overly value the &quot;100% code coverage&quot; data and forgot what real intention.</p><p>So, why is code coverage misleading? It is because 100% code coverage doesn&#39;t mean that all cases have been tested. But, what?? Yes, this may sound strange and let&#39;s look at the following code:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Access <span class="title">GetUserAccess</span><span class="params">(UserRole role)</span> </span>&#123;</span><br><span class="line">    Access access;</span><br><span class="line">    access.AddFlag(Access::Read);       <span class="comment">// Anyone can read.</span></span><br><span class="line">    <span class="keyword">if</span> (role &gt;= UserRole.Reader) &#123;      <span class="comment">// Bug!!!</span></span><br><span class="line">        access.AddFlag(Access::Write);  <span class="comment">// Only writers can write.</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> access;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The bug in this code is so obvious, but the following test with 100% code coverage cannot find it!</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Assert::AreEqual(Access::Read | Access::Write, GetUserAccess(UserRole.Writer));</span><br></pre></td></tr></table></figure><p>So only pursuing code coverage is no different from putting the cart before the horse. Then, what should we really go after? The answer always goes back to what our program really want to do, in other words - the requirements. The requirement can be as small as the purpose of a single class or as large as the customer scenario. The tests are written to ensure that our requirements are implemented correctly and do not regress. And this is called <strong>requirement coverage</strong>. Of course, it is hard to achieve high requirements coverage, because it requires us to have an good understanding of both our code and product to foresee what needs to be covered.</p><p>Besides foresight, hindsight is the equally important - <strong>defect coverage</strong>. Mistakes what were made before, we should add tests to ensure that the same mistakes will never happen again.</p><p>These two types of coverage are what we should really go after.</p><h3 id="Create-good-scaffolding">1.4. Create good scaffolding</h3><p>Some tests are hard to write directly. For example, although microservices are usually small enough to be easily covered by service tests, building them are still not easy. This is where we can help by building a test framework or utility, i.e. scaffolding. Just like the scaffolding in real life, it helps us to create an (easy) way to do what we want to do (test what we want to test).</p><p>In this example of testing microservices, we can use mock to simulate the communication layer of that service and provide generic mocks for upstream and downstream services. With this, testing our services will be much simpler. We can just simulate the requests via the mock upstream service and check if the state in our service and requests send to downstream serviceas are all looks good.</p><p>Scaffolding is also frequently used in large regression tests, integration tests and end-to-end tests. These tests usually require a certain environment to be built before testing, such as creating all the services and then simulating the customer requests and verifying the whole system. These works are usually very tedious. And having a unified and easy-to-use scaffolding can make the whole team more efficient.</p><p>If you are planning to creating a scaffolding, please <strong>treat it as a product</strong>! Amd here our customer are our internal developers. This means that we need to understand the requirements before implementation, and we need to collect user feedback to help us improve and iterate from time to time. Here are some principles on how to create good scaffolding. Hope it helps:</p><ul><li>The cost of using scaffolding must be lower than the cost of implementing our main logic for testing.<ul><li>Scaffolding is used to help us simplify testing.</li><li>If, after using scaffolding, we still see majority of the testing code is for building test environment, this scarffolding is definitely a failure.</li></ul></li><li>A scaffolding is a framework. It means it has responsibility to not only help simplify testing for everyone, but also help people avoid making mistakes.<ul><li>A good framework is a great helper as well as a constraint. The creator of any framework must be forward-thinking and help (or even force) everyone use the right approach to do things.</li><li>For example, if we use any actor model framework, it will be difficult to get data directly from one actor to another actor. Instead, we will have to send messages. This might sound annoying, but it&#39;s also one of the cornerstones of actor model that makes people hard to make mistakes. (Extended reading: <a href="https://youtu.be/PAAkCSZUG1c" target="_blank" rel="noopener">Go Proverbs: Don&#39;t communicate by sharing memory, share memory by communicating.</a>)</li></ul></li></ul><hr><h2 id="Metadata-based-testing">2. Metadata-based testing</h2><blockquote><p>&quot;Put Abstractions in Code, Details in Metadata&quot; - Andy Hunt, from &quot;The Pragmatic Programmer: Your Journey to Mastery&quot;</p></blockquote><p>In the <a href="/posts/pragmatic-testing-part-2#Make-behavior-changes-obvious">&quot;Make behavior changes obvious&quot;</a> section above, we encountered a problem: the more tests we create, the harder they are to maintain. This ended up discouraging us from writing tests. This is where metadata-based testing can really help.</p><h3 id="Extract-metadata">2.1. Extract metadata</h3><p>To help us simplify our tests, I recommend applying the idea of <a href="https://www.artima.com/intv/metadata.html" target="_blank" rel="noopener">separating application and metadata</a> when writing tests: Abstract the tests as much as possible into a unified scaffolding, then extract the details as metadata and use <a href="http://catb.org/~esr/writings/taoup/html/textualitychapter.html" target="_blank" rel="noopener">textuality storage</a>. In this way, if we need to write a new test case, we only need to add a new set of test metadata, the core testing logic doesn&#39;t need to be changed at all! And even better - the textualized metadata can be easily managed by any source control, and all the behavior changes can be revealed at a glance by simply checking the diff.</p><p>For example, the tedious code above can be simplified as follows:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">TEST_METHOD(MyService_WhenReceivingValidRequest_DownstreamServicesShouldBeProgrammed)</span><br><span class="line">&#123;</span><br><span class="line">    RunMyServiceStateHandlingTest(<span class="string">L"MyServiceTests\\ValidRequest-Input.json"</span>, <span class="string">L"MyServiceTests\\ValidRequest-ExpectedStatesAfterUpdate.json"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">StatesToTest</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    ServiceInternalState ServiceInternalState;</span><br><span class="line">    ServiceDownstreamStates StatesToDownstreamServices;</span><br><span class="line">    MyServiceTrackedStats TrackedStats;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">RunMyServiceStateHandlingTest</span><span class="params">(_In_ <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">wstring</span>&amp; inputFilePath, _In_ <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">wstring</span>&amp; expectedStatesAfterUpdateFilePath)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;MyService&gt; service = CreateMyService();</span><br><span class="line"></span><br><span class="line">    MyServiceRequest mockRequest;</span><br><span class="line">    JsonConverter::FromJsonFile(inputFilePath, mockRequest);</span><br><span class="line">    service-&gt;SendRequestSync(mockRequest);</span><br><span class="line"></span><br><span class="line">    StatesToTest actualStatesAfterUpdate;</span><br><span class="line">    actionStatesAfterUpdate.ServiceInternalState = service-&gt;GetInternalState();</span><br><span class="line">    actionStatesAfterUpdate.StatesToDownstreamServices = service-&gt;GetStatesToDownStreamServices();</span><br><span class="line">    actualStatesAfterUpdate.TrackedStats = service-&gt;GetStatsTracker()-&gt;GetTrackedStats();</span><br><span class="line"></span><br><span class="line">    StatesToTest expectedStatesAfterUpdate;</span><br><span class="line">    JsonConverter::FromJsonFile(expectedStatesAfterUpdateFilePath, expectedStatesAfterUpdate);</span><br><span class="line">    UnitTestUtils::AssertEquality(expectedStatesAfterUpdate, actualStatesAfterUpdate);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>After this change, all the details are extracted into metadata and saved in a json file. This not only makes the code shorter and better to read, also makes adding tests extremely easy. And if we made some behavior changes to our service, the test code doesn&#39;t need to be changed at all!</p><h3 id="Baseline-generation">2.2. Baseline generation</h3><p>Here, you may wonder, the workload is not really reduced at all, but just moved to changing the metadata file. So, what is the difference? Don&#39;t worry, because of this small change, a sea change is about to begin!</p><p>So, let&#39;s look at our metadata again. Do we really need to change it ourselves? No at all! To compare the states, we have to fetch the actual states as well as the expected states. So if we save the actual states as expected states, isn&#39;t this the metadata we want for future, i.e. a new baseline? So, we only need to make a very small change to make it work:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">RunMyServiceStateHandlingTest</span><span class="params">(_In_ <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">wstring</span>&amp; inputFilePath, _In_ <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">wstring</span>&amp; expectedStatesAfterUpdateFilePath)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="comment">// Replacing UnitTestUtils::AssertEquality(expectedStatesAfterUpdate, actualStatesAfterUpdate);</span></span><br><span class="line">    UnitTestUtils::GenerateBaselineOrAssertEquality(expectedStatesAfterUpdateFilePath, actualStatesAfterUpdate);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">UnitTestUtils</span>:</span>:GenerateBaselineOrAssertEquality(_In_ <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">wstring</span>&amp; expectedDataFilePath, _In_ <span class="keyword">const</span> T&amp; actualData);</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (UnitTestUtils::IsBaselineGenerationEnabled())</span><br><span class="line">    &#123;</span><br><span class="line">        JsonConverter::ToJsonFile(expectedDataFilePath, actualData);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    T expectedData;</span><br><span class="line">    JsonConverter::FromJsonFile(expectedDataFilePath, expectedData);</span><br><span class="line">    UnitTestUtils::AssertEquality(expectedStatesAfterUpdate, actualStatesAfterUpdate);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>There you go! With only a few lines of code change, now we have the ability of baseline generation. After we change the behavior of the service, we just need to turn on the switch for baseline generation and run all the tests again, all the metadata will be updated without changing a single line of testing code.</p><h3 id="Generating-reference-data-for-test-failures">2.3. Generating reference data for test failures</h3><p>That&#39;s not all. Another benefit this brings is that debugging becomes unbelievably easy! I don&#39;t know if you&#39;ve ever had the experience of debugging a test failure caused by two insanely complex objects (e.g. long/nested lists, structs/classses as follows) ......... So what failed the check? what else are different in the list besides the one fails the check? Who am I? Where am I? What am I doing here?</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Assert::AreEqual(longListWithDeeplyNestedStructs1, longListWithDeeplyNestedStructs2);</span><br></pre></td></tr></table></figure><p>And none of these issues are problems for metadata testing, because when test fails, we can generate reference data as well!</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">UnitTestUtils</span>:</span>:GenerateBaselineOrAssertEquality(_In_ <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">wstring</span>&amp; expectedDataFilePath, _In_ <span class="keyword">const</span> T&amp; actualData);</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (UnitTestUtils::IsBaselineGenerationEnabled())</span><br><span class="line">    &#123;</span><br><span class="line">        JsonConverter::ToJsonFile(expectedDataFilePath, actualData);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    T expectedData;</span><br><span class="line">    JsonConverter::FromJsonFile(expectedDataFilePath, expectedData);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span></span><br><span class="line">    &#123;</span><br><span class="line">        UnitTestUtils::AssertEquality(expectedStatesAfterUpdate, actualStatesAfterUpdate);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">catch</span> (...)</span><br><span class="line">    &#123;</span><br><span class="line">        UnitTestUtils::GenerateActualDataFileWhenTestFailed(actualData, expectedDataFilePath);</span><br><span class="line">        <span class="keyword">throw</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>After this small change, when the test fails, we will get the reference data in our log folder. And we can simply diff it against our baseline to see exactly what is changed. This will not just tell us the first failure, but gives us a full picture of the state change! It might be an ordering change, or some changes with a very obvious pattern. Knowing the full picture can greatly help us to figure out what might go wrong exactly.</p><p>Maybe you are wondering, can&#39;t we always generate the baseline and check diff for local development? For local develop, this is indeed possible, but sometimes some errors only occur on our build servers, which is when the reference data becomes quite useful. It greatly increases the observability of test failures.</p><h3 id="Good-enough-More-to-come">2.4. Good enough? More to come!</h3><p>I started experimenting this test approach since a few years ago and our team is currently using it. It has been proved to be very effective in reducing the development and maintenance costs of the tests. Hopefully, by this point, you will also start to get interested in metadata-based testing and willing to give it a try. However, its power doesn&#39;t stop there. But this post starts to become too long again, we&#39;ll stop here for now. In the next (and supposedly final) post, we&#39;ll discuss the advanced use of metadata-based testing and demonstrate how it can be used to better help us in our daily development.</p><hr><h2 id="quot-Fake-quot-tests">3. &quot;Fake&quot; tests</h2><p>When writing tests, we have to be especially careful about several types of tests below. Even if they existed, they wouldn&#39;t help us at all. In the best case, they will serve as placebo. But most of time, these tests are really harmful to our daily development.</p><h3 id="Flaky-tests">3.1. Flaky tests</h3><p>First and the worst, flaky tests. We mentioned it in <a href="/posts/pragmatic-testing-part-1#Avoid-unstable-code">&quot;Avoid unstable code&quot;</a> in the previous post. For this kind of test, we should disable it as soon as possible and treat it as a high priority development task. As long as the test is not fixed, development of the new features must stop. The reason is simple - if test cannot even pass, how do we know that the issue won&#39;t cause any problems for our customers?</p><p>The way to find this problem is quite simple and brutal, because the only way is to run the tests multiple times. So besides hearing multiple developers all complaining, we could either look at the test history (which will be mentioned in the next post) or just trigger a test stability test every so often and run each test many times to find the unstable ones.</p><p>I was once asked to fix such a test. Someone came to me and say: &quot;This test had about a 20% chance of failing a week ago, but this week it feels like it went up to 30%, can you see what&#39;s going on?&quot;. To be honest, I have no idea how to fix this. First of all, I don&#39;t know if it&#39;s because of <a href="https://www.youtube.com/watch?v=hLkQhaFOT58&ab_channel=BuzzFeedVideo" target="_blank" rel="noopener">mercury retrograde</a> causing you having a bad week and just simply being unlucky. Then, I don&#39;t know if the failure I&#39;m hitting is from the 20% that is &quot;ignorable&quot; and already exists, or the 10% that we need to fix? So, in the end, here is what I did:</p><ol><li>Disable this test completely, as whoever encounters its failure will just keep retry anyway. It is a total waste of our build and test resources.</li><li>Read this test and try to fully understand what it is actually trying to test.</li><li>Create a new test which can steadily reproduce the failure.</li><li>Fix the failure from this new test and commit it, then repeat steps 3-4 until all the problems are fixed.</li></ol><p>Finally, since the original test was written in a wrong way, I created a new test to test the scenario. After submitting it, I deleted the original test from our code base.</p><h3 id="Slow-tests">3.2. Slow tests</h3><p>Tests are supposed to help us find problems early, which means we should test as early and as often as possible. But if the tests are slow, we won&#39;t be able to do this. And ever worse, over time no one will run these tests anymore. Like the tests, which we mentioned <a href="/posts/pragmatic-testing-part-1#Increase-testable-surface-high-cohesion-low-coupling">in the previous post</a> and blindly sleep for 30 seconds everywhere, no one runs it in our project. When any test failed, people just simply retry or even comment them out! ...... So please do not ignore the performance of tests.</p><p>Of course, we should not pursue the speed of testing blindly (<a href="http://www.catb.org/esr/writings/taoup/html/ch01s06.html" target="_blank" rel="noopener">TAOUP: Rule of Economy</a>), but we will have different expectations for the speed of different types of tests. For example, large integration tests or end-to-end tests may take several hours to run, but we might only run such tests once a day for measuring the quality of our daily build. So, even if it takes an hour, it doesn&#39;t matter that much. However, it may not be ideal for such tests to take several days. For example, if you need a create a hotfix release to fix a urgent online issue, but after the build is done, the tests need three days to run to tell us whether the release is good or bad. By that time, I believe the customer will probably go crazy when hearing this.</p><p>An exception here is performance test. Performance test usually requires us to repeatly run a scenario many times, so it&#39;s natural that they take time. But again, this leads us to the same problem, they won&#39;t be run as ofthen as other tests. So, to ensure that other regular tests won&#39;t be impacted, we can isolate it out by putting they into a separate test class or test module.</p><h3 id="Shallow-Probes">3.3. Shallow Probes</h3><p>Probe is very helpful to building high availablity services. It provides to ability to check if an service is healthy or not. And if something goes wrong, it can help automatically triggers failover, fixes, rollback, or in the worse case - alerts. This is critical to get things automated, otherwise every service issue must be manually checked and fixed. And in large scale services, errors will happen. Without automation, our work will be extremely ineffiecient.</p><p>Many service governance frameworks provide probe support, for example, <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes" target="_blank" rel="noopener">kubernetes liveness, readiness, startup probe</a>. These probes provide convenient configuration and support for a variety of implementations, such as doing a tcp connect, or http request. These probes are intended to provide a convenient and uniform mechanism to check the health of a service, but this also leads to a very common problem: <strong>the things checked in the probe are way too simple!</strong></p><p>For example, a service is considered healthy as long as it is &quot;started&quot; successfully, e.g. process is running or certain port is open or certain core component is initialized. However, it is usually not enough for any service to properly serve any requests, which hides and real problems and gives us an impression that everything works fine, but actually not. And this brings up the real question: <strong>How healthy does a service have to be to be considered healthy?</strong> This is also the key ensuring the probe is implemented correctly.</p><p>But if we think about this question a bit carefully, we will quickly discover a very scary fact: <strong>A service has to be completely problem-free to be considered healthy!</strong> This means everything we need has to be correctly loaded, and not a single piece of customer data can go wrong (and the list goes on...)! Because only then, we can say the service should be able to correctly serve the requests now. This also means that the probe should check if everything of the service are running in the right state. I call this type of probe &quot;Deep Probe&quot;.</p><p>However, Deep probe also causes another problem: it takes too long to execute for a single probe request, which leads to timeout failures. So, the typical ways to implement deep probes are usually different from the regular probes:</p><ul><li><strong>Timers and health reporting</strong>: The idea is to move the deep probe logic out of the probe request handling into timers. In some service governance frameworks, service health checks is not using a pull model (timed probes), but a push model (health reporting), such as <a href="https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-health-introduction#health-reporting" target="_blank" rel="noopener">Service Fabric</a>. And when a problem is found, we can either report an unhealthy event or stop the heartbeat of the healthy event. And when using pull model, we can make our probe endpoint return failure when things goes wrong.</li><li><strong>Dedicated deep probe service</strong>: If the amount of data to be tested is too large, we can also create a dedicated deep probe service. The deep probe service can talk to other services and read in relevant information, then run tests and report errors. This makes it very easy to limit the overall resource usage and avoid impacting our key services. Of course, this also requires our services to have a good and unified service discovery and state discovery mechanism to support these operations.</li></ul><p>So, depends on how our services are implemented, we need to choose the right way to implement the deep probe. For example, sometimes it might not be a good idea to put deep probe in kubernetes readiness probe, as we might not want to the traffic to be cut for that partition. And creating a dedicated deep-probe service might be a better idea.</p><hr><h2 id="Let-39-s-take-another-break">4. Let&#39;s take another break</h2><p>Well, this post is a bit long again, so let&#39;s end here and call it part 2 and summarize everything we mentioned here again:</p><ol><li>First, we discussed how to write good tests:<ol><li>Choosing a reasonable way test our scenarios.</li><li>Giving reasonable hints about test failures, which include clarify test scenario by using assertive test names, make error messages actionable, and make behavior changes as obvious as possible.</li><li>Pursuing requirements coverage and defect coverage, rather than simple code coverage.</li><li>Build professional scaffolding to help us simplify the testing workload.</li></ol></li><li>To achieve the observability in behavior changes, maintaining tests becomes extremely expensive, so we introduced metadata-based testing and demonstrate how it helps greatly reduce the development and maintenance cost of test, improves even more on the observability of test failures, and improve the experience of debugging test failures.</li><li>Finally, we discuss several types of &quot;fake&quot; tests: flaky tests, slow tests and shallow probes. None of them helps but only being harmful to our daily development. And we also discussed the deep probe and how to implement it, to solve the shallow probe problem.</li></ol><p>In the next post, let&#39;s move on to discuss more about metadata-based testing and other test related topics.</p><hr><div class="post-series"><div class="post-series-title">Posts in the same series</div><ul class="post-series-list"><li class="post-series-list-item"><a href="http://en.r12f.com/posts/pragmatic-testing-part-2/">Pragmatic testing (P2: Good tests, Metadata-based testing, Fake tests)</a></li><li class="post-series-list-item"><a href="http://en.r12f.com/posts/pragmatic-testing-part-1/">Pragmatic testing (P1: Problems, principles and test-friendly code)</a></li></ul></div><b>Post link</b><a href="http://en.r12f.com/posts/pragmatic-testing-part-2/" target="_blank">Pragmatic testing (P2: Good tests, Metadata-based testing, Fake tests)</a></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Testing/" rel="tag"># Testing</a> <a href="/tags/Coding/" rel="tag"># Coding</a> <a href="/tags/Software-Engineering/" rel="tag"># Software Engineering</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/pragmatic-testing-part-1/" rel="prev" title="Pragmatic testing (P1: Problems, principles and test-friendly code)"><i class="fa fa-chevron-left"></i> Pragmatic testing (P1: Problems, principles and test-friendly code)</a></div><div class="post-nav-item"></div></div></footer></article></div></div><div class="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></div></div><script>window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">Table of Contents</li><li class="sidebar-nav-overview">Overview</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Good-tests"><span class="nav-text">1. Good tests</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Choose-the-right-way-to-test"><span class="nav-text">1.1. Choose the right way to test</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reasonable-prompt-messages"><span class="nav-text">1.2. Reasonable prompt messages</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Clarify-the-test-scenarios"><span class="nav-text">1.2.1. Clarify the test scenarios</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Actionable-error-messages"><span class="nav-text">1.2.2. Actionable error messages</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Make-behavior-changes-obvious"><span class="nav-text">1.2.3. Make behavior changes obvious</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Focus-on-requirement-and-defect-coverage"><span class="nav-text">1.3. Focus on requirement and defect coverage</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Create-good-scaffolding"><span class="nav-text">1.4. Create good scaffolding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Metadata-based-testing"><span class="nav-text">2. Metadata-based testing</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Extract-metadata"><span class="nav-text">2.1. Extract metadata</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Baseline-generation"><span class="nav-text">2.2. Baseline generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Generating-reference-data-for-test-failures"><span class="nav-text">2.3. Generating reference data for test failures</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Good-enough-More-to-come"><span class="nav-text">2.4. Good enough? More to come!</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#quot-Fake-quot-tests"><span class="nav-text">3. &quot;Fake&quot; tests</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Flaky-tests"><span class="nav-text">3.1. Flaky tests</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Slow-tests"><span class="nav-text">3.2. Slow tests</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Shallow-Probes"><span class="nav-text">3.3. Shallow Probes</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Let-39-s-take-another-break"><span class="nav-text">4. Let&#39;s take another break</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">r12f</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">2</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">1</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">3</span> <span class="site-state-item-name">tags</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/r12f" title="GitHub  https:&#x2F;&#x2F;github.com&#x2F;r12f" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://facebook.com/riffjiang" title="FB Page  https:&#x2F;&#x2F;facebook.com&#x2F;riffjiang" rel="noopener" target="_blank"><i class="fa fa-fw fa-facebook"></i>FB Page</a> </span><span class="links-of-author-item"><a href="/atom.xml" title="RSS  &#x2F;atom.xml"><i class="fa fa-fw fa-rss"></i>RSS</a></span></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"><a href="https://muqix.github.io/" title="https:&#x2F;&#x2F;muqix.github.io&#x2F;" rel="noopener" target="_blank">Muqi</a></li></ul></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">r12f</span></div><div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0</div><span class="post-meta-divider">|</span><div class="theme-info">Theme  <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.6.0</div><script>function leancloudSelector(url) {
    url = encodeURI(url);
    return document.getElementById(url).querySelector('.leancloud-visitors-count');
  }
  if (CONFIG.page.isPost) {
    function addCount(Counter) {
      var visitors = document.querySelector('.leancloud_visitors');
      var url = decodeURI(visitors.getAttribute('id'));
      var title = visitors.getAttribute('data-flag-title');

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length > 0) {
            var counter = results[0];
            Counter('put', '/classes/Counter/' + counter.objectId, { time: { '__op': 'Increment', 'amount': 1 } })
              .then(response => response.json())
              .then(() => {
                leancloudSelector(url).innerText = counter.time + 1;
              })
              .catch(error => {
                console.error('Failed to save visitor count', error);
              })
          } else {
              Counter('post', '/classes/Counter', { title: title, url: url, time: 1 })
                .then(response => response.json())
                .then(() => {
                  leancloudSelector(url).innerText = 1;
                })
                .catch(error => {
                  console.error('Failed to create', error);
                });
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }
  } else {
    function showTime(Counter) {
      var visitors = document.querySelectorAll('.leancloud_visitors');
      var entries = [...visitors].map(element => {
        return decodeURI(element.getAttribute('id'));
      });

      Counter('get', `/classes/Counter?where=${JSON.stringify({ url: { '$in': entries } })}`)
        .then(response => response.json())
        .then(({ results }) => {
          if (results.length === 0) {
            document.querySelectorAll('.leancloud_visitors .leancloud-visitors-count').forEach(element => {
              element.innerText = 0;
            });
            return;
          }
          for (let item of results) {
            let { url, time } = item;
            leancloudSelector(url).innerText = time;
          }
          for (let url of entries) {
            var element = leancloudSelector(url);
            if (element.innerText == '') {
              element.innerText = 0;
            }
          }
        })
        .catch(error => {
          console.error('LeanCloud Counter Error', error);
        });
    }
  }

  fetch('https://app-router.leancloud.cn/2/route?appId=aanH10fNtc25mqFK1gSRqjo9-gzGzoHsz')
    .then(response => response.json())
    .then(({ api_server }) => {
      var Counter = (method, url, data) => {
        return fetch(`https://${api_server}/1.1${url}`, {
          method: method,
          headers: {
            'X-LC-Id': 'aanH10fNtc25mqFK1gSRqjo9-gzGzoHsz',
            'X-LC-Key': '4AhjDuBCXDyUgpY0Cvh4wEol',
            'Content-Type': 'application/json',
          },
          body: JSON.stringify(data)
        });
      };
      if (CONFIG.page.isPost) {
        if (CONFIG.hostname !== location.hostname) return;
        addCount(Counter);
      } else if (document.querySelectorAll('.post-title-link').length >= 1) {
        showTime(Counter);
      }
    });</script></div></footer></div><script src="/lib/anime.min.js"></script><script src="/js/utils.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script>function loadCount(){var d=document,n=d.createElement("script");n.src="https://r12f-blog-en.disqus.com/count.js",n.id="dsq-count-scr",(d.head||d.body).appendChild(n)}window.addEventListener("load",loadCount,!1)</script><script>var disqus_config = function() {
    this.page.url = "http://en.r12f.com/posts/pragmatic-testing-part-2/";
    this.page.identifier = "posts/pragmatic-testing-part-2/";
    this.page.title = "Pragmatic testing (P2: Good tests, Metadata-based testing, Fake tests)";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://r12f-blog-en.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });</script></body></html>