<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Soul Orbit</title>
  
  <subtitle>I&#39;ll take a quiet life. A handshake of carbon monoxide.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://en.r12f.com/"/>
  <updated>2021-02-17T04:15:41.358Z</updated>
  <id>http://en.r12f.com/</id>
  
  <author>
    <name>r12f</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Pragmatic testing (P2: Good tests, Metadata-based testing, Fake tests)</title>
    <link href="http://en.r12f.com/posts/pragmatic-testing-part-2/"/>
    <id>http://en.r12f.com/posts/pragmatic-testing-part-2/</id>
    <published>2021-02-15T22:56:13.000Z</published>
    <updated>2021-02-17T04:15:41.358Z</updated>
    
    <content type="html"><![CDATA[<p><a href="/posts/pragmatic-testing-part-1">In the last post, we discussed the reasons for writing tests, principles and how to write code that is easy to test</a>. In this post, we&#39;ll discuss more on how to write good tests.</p><p>First, let me repeat the important thing: <strong>Test is a not a silver bullet.</strong> Adding a few tests won&#39;t help us improve the code quality that much. If you haven&#39;t read the <a href="/posts/pragmatic-testing-part-1">part 1</a> yet, it is highly recommended to read it first.</p><hr><h2 id="Good-tests">1. Good tests</h2><p>We are finally here! Let&#39;s talk about writing tests! Making code easy to test is only the first step, and to write good tests we will need some more skills.</p><h3 id="Choose-the-right-way-to-test">1.1. Choose the right way to test</h3><p>We know that there are various types of tests. And here are some examples:</p><ul><li><strong>Unit test</strong>: Test a class or a small module by talking to them directly, e.g. calling public member functions.</li><li><strong>Module/Service test</strong>: Test a module or service by simulating its upstream and downstream services.</li><li><strong>End-To-End test (E2E test)</strong>: Test a complete system by simulating the upstream and downstream services of the whole system.</li><li><strong>Integration test</strong>: Integrate all services, even multiple systems, as required and test them.</li><li><strong>In-Service/Probe test</strong>: Test the services within the service itself or with a dedicated probe service at regular intervals.</li></ul><a id="more"></a><p>Each of these tests has its own advantages and disadvantages, roughly as follows.</p><table><thead><tr><th>Test type</th><th>Implementation difficulty</th><th>Local testing?</th><th>Test speed</th><th>Time to find problems</th><th>Complexity of finding problems</th><th>Difficulty of debugging problems</th></tr></thead><tbody><tr><td>Unit Testing</td><td>Easy</td><td>Yes</td><td>Fast</td><td>Early</td><td>Simple</td><td>Easy</td></tr><tr><td>Module/Service Testing</td><td>Relatively Easy</td><td>Doable</td><td>Relatively Fast</td><td>Relatively Early</td><td>Normal</td><td>Relatively Simple<br>(if local debugging is possible)</td></tr><tr><td>End-to-end testing</td><td>Normal</td><td>Doable</td><td>Nromal</td><td>Normal</td><td>Relatively Complex</td><td>Relatively Simple<br>(if local debugging is possible)</td></tr><tr><td>Integration Testing</td><td>Hard</td><td>Usually No</td><td>Slow</td><td>Late</td><td>Complex</td><td>Hard</td></tr><tr><td>Probe Testing</td><td>Normal</td><td>Usually No</td><td>Relatively Fast</td><td>Late</td><td>Normal</td><td>Relatively Hard</td></tr></tbody></table><p>Therefore, when writing tests, we need to think about what we want to test and choose the appropriate way to test it.</p><p>When choosing, we must pay attention to the <a href="/posts/pragmatic-testing-part-1Principles-of-testing">shortest distance principle</a>. For example, we may feel that the integration test will have the best coverage, so we should test everything with integration tests. But precisely because of its coverage, we may spend a tremendous amount of work building these tests, and debugging them when test fails. Imagine an exception is thrown in a very strange place after dozens of services have called each other hundreds of times. And then, we need to debug this and find out why... (Good luck with this :D)</p><hr><h3 id="Reasonable-prompt-messages">1.2. Reasonable prompt messages</h3><blockquote><p>&quot;Transparency is a passive quality. A program is transparent when it is possible to form a simple mental model of its behavior that is actually predictive for all or most cases, because you can see through the machinery to what is actually going on.&quot;</p><p>- Eric Steven Raymond, from &quot;The Art of Unix Programming&quot;</p></blockquote><p>While tests can tell us that something is wrong, many people often miss the point that: <strong>it is more important to tell us what is wrong and what to check</strong>.</p><h4 id="Clarify-the-test-scenarios">1.2.1. Clarify the test scenarios</h4><p>The first and most overlooked thing is the names of the tests. Here are some examples that I&#39;ve (often) seen in real projects:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Test</span><br><span class="line">Test1/<span class="number">2</span>/<span class="number">3</span>/...</span><br><span class="line">TestSwitch</span><br><span class="line">TestBasic</span><br><span class="line">MyTest</span><br><span class="line">FooTest/TestFoo <span class="comment">// Foo is the name of the class that being tested</span></span><br></pre></td></tr></table></figure><p>These test names won&#39;t help us understand their purpose at all, so when something goes wrong we have no idea what to look at. A better way here would be to <strong>describe the test scenario well and use assertive test name</strong>.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ScopeHandle_AfterDtor_UnderlyingHandleShouldBeClosed</span><br><span class="line">CrontabTimer_ParsingValidCrontabSyntax_ShouldSucceed</span><br><span class="line">DataProcessor_ProcessingDataWithUnexpectedFormat_ShouldFail</span><br><span class="line">HealthProber_ProbeUnresponsiveHttpTarget_ShouldFailWithTimeout</span><br></pre></td></tr></table></figure><p>Don&#39;t worry about long function names. Being descriptive is never wrong.</p><h4 id="Actionable-error-messages">1.2.2. Actionable error messages</h4><p>When test fails, the error message must be clear and actionable.</p><p>For example: when writing assertions, don&#39;t just simply write something like <code>Assert(a != 0);</code>. It is better to give some advice:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Assert(dataIterator != dataMap.<span class="built_in">end</span>(),</span><br><span class="line">       <span class="string">"Data is not created while id still exists. It might be a leak on id. Please check CreateData/RemoveData functions."</span>);</span><br></pre></td></tr></table></figure><p>Debugging such error will be much easier because the error is very clear and specific.</p><h4 id="Make-behavior-changes-obvious">1.2.3. Make behavior changes obvious</h4><p>As we mentioned before, one of the purpose of testing is to identify behavior changes. So, to ensure high observability, these behavior changes must be as clear and obvious as possible. These include changes in our service internal states, data reporting, and even behavior of debugging related tools (e.g., APIs for getting certain state of the service).</p><p>This idea is great, but it also leads to the problem - If we want to see all the behavior changes, we have to write code to test every behavior our program. This makes it extremely hard to maintain these tests!</p><p>Here&#39;s a simple example: <a href="https://www.oreilly.com/library/view/building-microservices/9781491950340/ch07.html#idp3960144" target="_blank" rel="noopener">Testing a microservice by simulating a service upstream and downstream</a> is a very common practise, so I&#39;m sure you&#39;ve seen countless of tests that look like this:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">TEST_METHOD(MyService_WhenReceivingValidRequest_DownstreamServicesShouldBeProgrammed)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;MyService&gt; service = CreateMyService();</span><br><span class="line"></span><br><span class="line">    MyServiceRequest mockRequest;</span><br><span class="line">    mockRequest.Foo = <span class="number">1</span>;</span><br><span class="line">    mockRequest.Bar = <span class="number">2</span>;</span><br><span class="line">    <span class="comment">// ... Update all fields here.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Simulating upstream service sending state to my service</span></span><br><span class="line">    service-&gt;SendRequestSync(mockRequest);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Test service internal states</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;<span class="keyword">const</span> MyServiceState&gt; serviceState = service-&gt;GetInternalState()-&gt;GetPayload&lt;MyServiceState&gt;();</span><br><span class="line">    Assert::AreEqual(<span class="number">1</span>, serviceState-&gt;Foo);</span><br><span class="line">    Assert::AreEqual(<span class="number">2</span>, serviceState-&gt;Bar);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Validate downstream services</span></span><br><span class="line">    ServiceDownstreamStates statesToDownstreamServices = service-&gt;GetStatesToDownStreamServices();</span><br><span class="line">    Assert::AreEqual(<span class="number">2u</span>ll, statesToDownstreamServices.<span class="built_in">size</span>());</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Validate states send to downstream service 1</span></span><br><span class="line">    Assert::IsTrue(statesToDownstreamServices.<span class="built_in">find</span>(<span class="string">L"MyDownstreamService1"</span>) != statesToDownstreamServices.<span class="built_in">end</span>());</span><br><span class="line">    Assert::AreEqual(<span class="number">1u</span>ll, statesToDownstreamServices[<span class="string">L"MyDownstreamService1"</span>].States.<span class="built_in">size</span>());</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;<span class="keyword">const</span> MyDownstreamService1Request&gt; stateToDownstreamService1 = statesToDownstreamServices[<span class="string">L"MyDownstreamService1"</span>].States[<span class="number">0</span>].GetPayload&lt;MyDownstreamService1Request&gt;();</span><br><span class="line">    Assert::IsNotNull(stateToDownstreamService1);</span><br><span class="line">    Assert::AreEqual(<span class="number">1</span>, stateToDownstreamService1-&gt;Foo);</span><br><span class="line">    <span class="comment">// ... Validate all other states</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Validate states send to downstream service 2</span></span><br><span class="line">    Assert::IsTrue(statesToDownstreamServices.<span class="built_in">find</span>(<span class="string">L"MyDownstreamService2"</span>) != statesToDownstreamServices.<span class="built_in">end</span>());</span><br><span class="line">    Assert::AreEqual(<span class="number">1u</span>ll, statesToDownstreamServices[<span class="string">L"MyDownstreamService2"</span>].States.<span class="built_in">size</span>());</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;<span class="keyword">const</span> MyDownstreamService2Request&gt; stateToDownstreamService2 = statesToDownstreamServices[<span class="string">L"MyDownstreamService2"</span>].States[<span class="number">0</span>].GetPayload&lt;MyDownstreamService2Request&gt;();</span><br><span class="line">    Assert::IsNotNull(stateToDownstreamService2);</span><br><span class="line">    Assert::AreEqual(<span class="number">2</span>, stateToDownstreamService2-&gt;Bar);</span><br><span class="line">    <span class="comment">// ... Validate all other states</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Validate metrics</span></span><br><span class="line">    MyServiceTrackedStats trackedStats = service-&gt;GetStatsTracker()-&gt;GetTrackedStats();</span><br><span class="line">    Assert::AreEqual(<span class="number">1</span>, trackedStates.RequestCount);</span><br><span class="line">    Assert::AreEqual(<span class="number">1</span>, trackedStates.Service1RequestSent);</span><br><span class="line">    Assert::AreEqual(<span class="number">1</span>, trackedStates.Service2RequestSent);</span><br><span class="line">    <span class="comment">// ... Validate all other stats</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>As we can see, even with the help from a lot of helper functions to simplify the code, the tests are still very tedious to write. And once any behavior change is made, all the tests must be changed along with it. This leads to an extremely high workload in maintaining the tests, which makes everyone so tired of adding more. So, is there a way to have the best of both worlds? Gladly, yes! Later, we will introduce metadata-based testing to help us solve this problem.</p><h3 id="Focus-on-requirement-and-defect-coverage">1.3. Focus on requirement and defect coverage</h3><p>Nowadays, many testing tools can provide code coverage. And high code coverage is also enforced in many projects. While this indeed helps, it can also mislead people, making people overly value the &quot;100% code coverage&quot; data and forgot what real intention.</p><p>So, why is code coverage misleading? It is because 100% code coverage doesn&#39;t mean that all cases have been tested. But, what?? Yes, this may sound strange and let&#39;s look at the following code:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Access <span class="title">GetUserAccess</span><span class="params">(UserRole role)</span> </span>&#123;</span><br><span class="line">    Access access;</span><br><span class="line">    access.AddFlag(Access::Read);       <span class="comment">// Anyone can read.</span></span><br><span class="line">    <span class="keyword">if</span> (role &gt;= UserRole.Reader) &#123;      <span class="comment">// Bug!!!</span></span><br><span class="line">        access.AddFlag(Access::Write);  <span class="comment">// Only writers can write.</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> access;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The bug in this code is so obvious, but the following test with 100% code coverage cannot find it!</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Assert::AreEqual(Access::Read | Access::Write, GetUserAccess(UserRole.Writer));</span><br></pre></td></tr></table></figure><p>So only pursuing code coverage is no different from putting the cart before the horse. Then, what should we really go after? The answer always goes back to what our program really want to do, in other words - the requirements. The requirement can be as small as the purpose of a single class or as large as the customer scenario. The tests are written to ensure that our requirements are implemented correctly and do not regress. And this is called <strong>requirement coverage</strong>. Of course, it is hard to achieve high requirements coverage, because it requires us to have an good understanding of both our code and product to foresee what needs to be covered.</p><p>Besides foresight, hindsight is the equally important - <strong>defect coverage</strong>. Mistakes what were made before, we should add tests to ensure that the same mistakes will never happen again.</p><p>These two types of coverage are what we should really go after.</p><h3 id="Create-good-scaffolding">1.4. Create good scaffolding</h3><p>Some tests are hard to write directly. For example, although microservices are usually small enough to be easily covered by service tests, building them are still not easy. This is where we can help by building a test framework or utility, i.e. scaffolding. Just like the scaffolding in real life, it helps us to create an (easy) way to do what we want to do (test what we want to test).</p><p>In this example of testing microservices, we can use mock to simulate the communication layer of that service and provide generic mocks for upstream and downstream services. With this, testing our services will be much simpler. We can just simulate the requests via the mock upstream service and check if the state in our service and requests send to downstream serviceas are all looks good.</p><p>Scaffolding is also frequently used in large regression tests, integration tests and end-to-end tests. These tests usually require a certain environment to be built before testing, such as creating all the services and then simulating the customer requests and verifying the whole system. These works are usually very tedious. And having a unified and easy-to-use scaffolding can make the whole team more efficient.</p><p>If you are planning to creating a scaffolding, please <strong>treat it as a product</strong>! Amd here our customer are our internal developers. This means that we need to understand the requirements before implementation, and we need to collect user feedback to help us improve and iterate from time to time. Here are some principles on how to create good scaffolding. Hope it helps:</p><ul><li>The cost of using scaffolding must be lower than the cost of implementing our main logic for testing.<ul><li>Scaffolding is used to help us simplify testing.</li><li>If, after using scaffolding, we still see majority of the testing code is for building test environment, this scarffolding is definitely a failure.</li></ul></li><li>A scaffolding is a framework. It means it has responsibility to not only help simplify testing for everyone, but also help people avoid making mistakes.<ul><li>A good framework is a great helper as well as a constraint. The creator of any framework must be forward-thinking and help (or even force) everyone use the right approach to do things.</li><li>For example, if we use any actor model framework, it will be difficult to get data directly from one actor to another actor. Instead, we will have to send messages. This might sound annoying, but it&#39;s also one of the cornerstones of actor model that makes people hard to make mistakes. (Extended reading: <a href="https://youtu.be/PAAkCSZUG1c" target="_blank" rel="noopener">Go Proverbs: Don&#39;t communicate by sharing memory, share memory by communicating.</a>)</li></ul></li></ul><hr><h2 id="Metadata-based-testing">2. Metadata-based testing</h2><blockquote><p>&quot;Put Abstractions in Code, Details in Metadata&quot; - Andy Hunt, from &quot;The Pragmatic Programmer: Your Journey to Mastery&quot;</p></blockquote><p>In the <a href="/posts/pragmatic-testing-part-2#Make-behavior-changes-obvious">&quot;Make behavior changes obvious&quot;</a> section above, we encountered a problem: the more tests we create, the harder they are to maintain. This ended up discouraging us from writing tests. This is where metadata-based testing can really help.</p><h3 id="Extract-metadata">2.1. Extract metadata</h3><p>To help us simplify our tests, I recommend applying the idea of <a href="https://www.artima.com/intv/metadata.html" target="_blank" rel="noopener">separating application and metadata</a> when writing tests: Abstract the tests as much as possible into a unified scaffolding, then extract the details as metadata and use <a href="http://catb.org/~esr/writings/taoup/html/textualitychapter.html" target="_blank" rel="noopener">textuality storage</a>. In this way, if we need to write a new test case, we only need to add a new set of test metadata, the core testing logic doesn&#39;t need to be changed at all! And even better - the textualized metadata can be easily managed by any source control, and all the behavior changes can be revealed at a glance by simply checking the diff.</p><p>For example, the tedious code above can be simplified as follows:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">TEST_METHOD(MyService_WhenReceivingValidRequest_DownstreamServicesShouldBeProgrammed)</span><br><span class="line">&#123;</span><br><span class="line">    RunMyServiceStateHandlingTest(<span class="string">L"MyServiceTests\\ValidRequest-Input.json"</span>, <span class="string">L"MyServiceTests\\ValidRequest-ExpectedStatesAfterUpdate.json"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">StatesToTest</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    ServiceInternalState ServiceInternalState;</span><br><span class="line">    ServiceDownstreamStates StatesToDownstreamServices;</span><br><span class="line">    MyServiceTrackedStats TrackedStats;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">RunMyServiceStateHandlingTest</span><span class="params">(_In_ <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">wstring</span>&amp; inputFilePath, _In_ <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">wstring</span>&amp; expectedStatesAfterUpdateFilePath)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;MyService&gt; service = CreateMyService();</span><br><span class="line"></span><br><span class="line">    MyServiceRequest mockRequest;</span><br><span class="line">    JsonConverter::FromJsonFile(inputFilePath, mockRequest);</span><br><span class="line">    service-&gt;SendRequestSync(mockRequest);</span><br><span class="line"></span><br><span class="line">    StatesToTest actualStatesAfterUpdate;</span><br><span class="line">    actionStatesAfterUpdate.ServiceInternalState = service-&gt;GetInternalState();</span><br><span class="line">    actionStatesAfterUpdate.StatesToDownstreamServices = service-&gt;GetStatesToDownStreamServices();</span><br><span class="line">    actualStatesAfterUpdate.TrackedStats = service-&gt;GetStatsTracker()-&gt;GetTrackedStats();</span><br><span class="line"></span><br><span class="line">    StatesToTest expectedStatesAfterUpdate;</span><br><span class="line">    JsonConverter::FromJsonFile(expectedStatesAfterUpdateFilePath, expectedStatesAfterUpdate);</span><br><span class="line">    UnitTestUtils::AssertEquality(expectedStatesAfterUpdate, actualStatesAfterUpdate);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>After this change, all the details are extracted into metadata and saved in a json file. This not only makes the code shorter and better to read, also makes adding tests extremely easy. And if we made some behavior changes to our service, the test code doesn&#39;t need to be changed at all!</p><h3 id="Baseline-generation">2.2. Baseline generation</h3><p>Here, you may wonder, the workload is not really reduced at all, but just moved to changing the metadata file. So, what is the difference? Don&#39;t worry, because of this small change, a sea change is about to begin!</p><p>So, let&#39;s look at our metadata again. Do we really need to change it ourselves? No at all! To compare the states, we have to fetch the actual states as well as the expected states. So if we save the actual states as expected states, isn&#39;t this the metadata we want for future, i.e. a new baseline? So, we only need to make a very small change to make it work:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">RunMyServiceStateHandlingTest</span><span class="params">(_In_ <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">wstring</span>&amp; inputFilePath, _In_ <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">wstring</span>&amp; expectedStatesAfterUpdateFilePath)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="comment">// Replacing UnitTestUtils::AssertEquality(expectedStatesAfterUpdate, actualStatesAfterUpdate);</span></span><br><span class="line">    UnitTestUtils::GenerateBaselineOrAssertEquality(expectedStatesAfterUpdateFilePath, actualStatesAfterUpdate);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">UnitTestUtils</span>:</span>:GenerateBaselineOrAssertEquality(_In_ <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">wstring</span>&amp; expectedDataFilePath, _In_ <span class="keyword">const</span> T&amp; actualData);</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (UnitTestUtils::IsBaselineGenerationEnabled())</span><br><span class="line">    &#123;</span><br><span class="line">        JsonConverter::ToJsonFile(expectedDataFilePath, actualData);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    T expectedData;</span><br><span class="line">    JsonConverter::FromJsonFile(expectedDataFilePath, expectedData);</span><br><span class="line">    UnitTestUtils::AssertEquality(expectedStatesAfterUpdate, actualStatesAfterUpdate);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>There you go! With only a few lines of code change, now we have the ability of baseline generation. After we change the behavior of the service, we just need to turn on the switch for baseline generation and run all the tests again, all the metadata will be updated without changing a single line of testing code.</p><h3 id="Generating-reference-data-for-test-failures">2.3. Generating reference data for test failures</h3><p>That&#39;s not all. Another benefit this brings is that debugging becomes unbelievably easy! I don&#39;t know if you&#39;ve ever had the experience of debugging a test failure caused by two insanely complex objects (e.g. long/nested lists, structs/classses as follows) ......... So what failed the check? what else are different in the list besides the one fails the check? Who am I? Where am I? What am I doing here?</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Assert::AreEqual(longListWithDeeplyNestedStructs1, longListWithDeeplyNestedStructs2);</span><br></pre></td></tr></table></figure><p>And none of these issues are problems for metadata testing, because when test fails, we can generate reference data as well!</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">UnitTestUtils</span>:</span>:GenerateBaselineOrAssertEquality(_In_ <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">wstring</span>&amp; expectedDataFilePath, _In_ <span class="keyword">const</span> T&amp; actualData);</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">if</span> (UnitTestUtils::IsBaselineGenerationEnabled())</span><br><span class="line">    &#123;</span><br><span class="line">        JsonConverter::ToJsonFile(expectedDataFilePath, actualData);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    T expectedData;</span><br><span class="line">    JsonConverter::FromJsonFile(expectedDataFilePath, expectedData);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span></span><br><span class="line">    &#123;</span><br><span class="line">        UnitTestUtils::AssertEquality(expectedStatesAfterUpdate, actualStatesAfterUpdate);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">catch</span> (...)</span><br><span class="line">    &#123;</span><br><span class="line">        UnitTestUtils::GenerateActualDataFileWhenTestFailed(actualData, expectedDataFilePath);</span><br><span class="line">        <span class="keyword">throw</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>After this small change, when the test fails, we will get the reference data in our log folder. And we can simply diff it against our baseline to see exactly what is changed. This will not just tell us the first failure, but gives us a full picture of the state change! It might be an ordering change, or some changes with a very obvious pattern. Knowing the full picture can greatly help us to figure out what might go wrong exactly.</p><p>Maybe you are wondering, can&#39;t we always generate the baseline and check diff for local development? For local develop, this is indeed possible, but sometimes some errors only occur on our build servers, which is when the reference data becomes quite useful. It greatly increases the observability of test failures.</p><h3 id="Good-enough-More-to-come">2.4. Good enough? More to come!</h3><p>I started experimenting this test approach since a few years ago and our team is currently using it. It has been proved to be very effective in reducing the development and maintenance costs of the tests. Hopefully, by this point, you will also start to get interested in metadata-based testing and willing to give it a try. However, its power doesn&#39;t stop there. But this post starts to become too long again, we&#39;ll stop here for now. In the next (and supposedly final) post, we&#39;ll discuss the advanced use of metadata-based testing and demonstrate how it can be used to better help us in our daily development.</p><hr><h2 id="quot-Fake-quot-tests">3. &quot;Fake&quot; tests</h2><p>When writing tests, we have to be especially careful about several types of tests below. Even if they existed, they wouldn&#39;t help us at all. In the best case, they will serve as placebo. But most of time, these tests are really harmful to our daily development.</p><h3 id="Flaky-tests">3.1. Flaky tests</h3><p>First and the worst, flaky tests. We mentioned it in <a href="/posts/pragmatic-testing-part-1#Avoid-unstable-code">&quot;Avoid unstable code&quot;</a> in the previous post. For this kind of test, we should disable it as soon as possible and treat it as a high priority development task. As long as the test is not fixed, development of the new features must stop. The reason is simple - if test cannot even pass, how do we know that the issue won&#39;t cause any problems for our customers?</p><p>The way to find this problem is quite simple and brutal, because the only way is to run the tests multiple times. So besides hearing multiple developers all complaining, we could either look at the test history (which will be mentioned in the next post) or just trigger a test stability test every so often and run each test many times to find the unstable ones.</p><p>I was once asked to fix such a test. Someone came to me and say: &quot;This test had about a 20% chance of failing a week ago, but this week it feels like it went up to 30%, can you see what&#39;s going on?&quot;. To be honest, I have no idea how to fix this. First of all, I don&#39;t know if it&#39;s because of <a href="https://www.youtube.com/watch?v=hLkQhaFOT58&ab_channel=BuzzFeedVideo" target="_blank" rel="noopener">mercury retrograde</a> causing you having a bad week and just simply being unlucky. Then, I don&#39;t know if the failure I&#39;m hitting is from the 20% that is &quot;ignorable&quot; and already exists, or the 10% that we need to fix? So, in the end, here is what I did:</p><ol><li>Disable this test completely, as whoever encounters its failure will just keep retry anyway. It is a total waste of our build and test resources.</li><li>Read this test and try to fully understand what it is actually trying to test.</li><li>Create a new test which can steadily reproduce the failure.</li><li>Fix the failure from this new test and commit it, then repeat steps 3-4 until all the problems are fixed.</li></ol><p>Finally, since the original test was written in a wrong way, I created a new test to test the scenario. After submitting it, I deleted the original test from our code base.</p><h3 id="Slow-tests">3.2. Slow tests</h3><p>Tests are supposed to help us find problems early, which means we should test as early and as often as possible. But if the tests are slow, we won&#39;t be able to do this. And ever worse, over time no one will run these tests anymore. Like the tests, which we mentioned <a href="/posts/pragmatic-testing-part-1#Increase-testable-surface-high-cohesion-low-coupling">in the previous post</a> and blindly sleep for 30 seconds everywhere, no one runs it in our project. When any test failed, people just simply retry or even comment them out! ...... So please do not ignore the performance of tests.</p><p>Of course, we should not pursue the speed of testing blindly (<a href="http://www.catb.org/esr/writings/taoup/html/ch01s06.html" target="_blank" rel="noopener">TAOUP: Rule of Economy</a>), but we will have different expectations for the speed of different types of tests. For example, large integration tests or end-to-end tests may take several hours to run, but we might only run such tests once a day for measuring the quality of our daily build. So, even if it takes an hour, it doesn&#39;t matter that much. However, it may not be ideal for such tests to take several days. For example, if you need a create a hotfix release to fix a urgent online issue, but after the build is done, the tests need three days to run to tell us whether the release is good or bad. By that time, I believe the customer will probably go crazy when hearing this.</p><p>An exception here is performance test. Performance test usually requires us to repeatly run a scenario many times, so it&#39;s natural that they take time. But again, this leads us to the same problem, they won&#39;t be run as ofthen as other tests. So, to ensure that other regular tests won&#39;t be impacted, we can isolate it out by putting they into a separate test class or test module.</p><h3 id="Shallow-Probes">3.3. Shallow Probes</h3><p>Probe is very helpful to building high availablity services. It provides to ability to check if an service is healthy or not. And if something goes wrong, it can help automatically triggers failover, fixes, rollback, or in the worse case - alerts. This is critical to get things automated, otherwise every service issue must be manually checked and fixed. And in large scale services, errors will happen. Without automation, our work will be extremely ineffiecient.</p><p>Many service governance frameworks provide probe support, for example, <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes" target="_blank" rel="noopener">kubernetes liveness, readiness, startup probe</a>. These probes provide convenient configuration and support for a variety of implementations, such as doing a tcp connect, or http request. These probes are intended to provide a convenient and uniform mechanism to check the health of a service, but this also leads to a very common problem: <strong>the things checked in the probe are way too simple!</strong></p><p>For example, a service is considered healthy as long as it is &quot;started&quot; successfully, e.g. process is running or certain port is open or certain core component is initialized. However, it is usually not enough for any service to properly serve any requests, which hides and real problems and gives us an impression that everything works fine, but actually not. And this brings up the real question: <strong>How healthy does a service have to be to be considered healthy?</strong> This is also the key ensuring the probe is implemented correctly.</p><p>But if we think about this question a bit carefully, we will quickly discover a very scary fact: <strong>A service has to be completely problem-free to be considered healthy!</strong> This means everything we need has to be correctly loaded, and not a single piece of customer data can go wrong (and the list goes on...)! Because only then, we can say the service should be able to correctly serve the requests now. This also means that the probe should check if everything of the service are running in the right state. I call this type of probe &quot;Deep Probe&quot;.</p><p>However, Deep probe also causes another problem: it takes too long to execute for a single probe request, which leads to timeout failures. So, the typical ways to implement deep probes are usually different from the regular probes:</p><ul><li><strong>Timers and health reporting</strong>: The idea is to move the deep probe logic out of the probe request handling into timers. In some service governance frameworks, service health checks is not using a pull model (timed probes), but a push model (health reporting), such as <a href="https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-health-introduction#health-reporting" target="_blank" rel="noopener">Service Fabric</a>. And when a problem is found, we can either report an unhealthy event or stop the heartbeat of the healthy event. And when using pull model, we can make our probe endpoint return failure when things goes wrong.</li><li><strong>Dedicated deep probe service</strong>: If the amount of data to be tested is too large, we can also create a dedicated deep probe service. The deep probe service can talk to other services and read in relevant information, then run tests and report errors. This makes it very easy to limit the overall resource usage and avoid impacting our key services. Of course, this also requires our services to have a good and unified service discovery and state discovery mechanism to support these operations.</li></ul><p>So, depends on how our services are implemented, we need to choose the right way to implement the deep probe. For example, sometimes it might not be a good idea to put deep probe in kubernetes readiness probe, as we might not want to the traffic to be cut for that partition. And creating a dedicated deep-probe service might be a better idea.</p><hr><h2 id="Let-39-s-take-another-break">4. Let&#39;s take another break</h2><p>Well, this post is a bit long again, so let&#39;s end here and call it part 2 and summarize everything we mentioned here again:</p><ol><li>First, we discussed how to write good tests:<ol><li>Choosing a reasonable way test our scenarios.</li><li>Giving reasonable hints about test failures, which include clarify test scenario by using assertive test names, make error messages actionable, and make behavior changes as obvious as possible.</li><li>Pursuing requirements coverage and defect coverage, rather than simple code coverage.</li><li>Build professional scaffolding to help us simplify the testing workload.</li></ol></li><li>To achieve the observability in behavior changes, maintaining tests becomes extremely expensive, so we introduced metadata-based testing and demonstrate how it helps greatly reduce the development and maintenance cost of test, improves even more on the observability of test failures, and improve the experience of debugging test failures.</li><li>Finally, we discuss several types of &quot;fake&quot; tests: flaky tests, slow tests and shallow probes. None of them helps but only being harmful to our daily development. And we also discussed the deep probe and how to implement it, to solve the shallow probe problem.</li></ol><p>In the next post, let&#39;s move on to discuss more about metadata-based testing and other test related topics.</p><hr><div class="post-series"><div class="post-series-title">Posts in the same series：</div><ul class="post-series-list"><li class="post-series-list-item"><a href="http://en.r12f.com/posts/pragmatic-testing-part-2/">Pragmatic testing (P2: Good tests, Metadata-based testing, Fake tests)</a></li><li class="post-series-list-item"><a href="http://en.r12f.com/posts/pragmatic-testing-part-1/">Pragmatic testing (P1: Problems, principles and test-friendly code)</a></li></ul></div><b>Post link：</b><a href="http://en.r12f.com/posts/pragmatic-testing-part-2/" target="_blank">Pragmatic testing (P2: Good tests, Metadata-based testing, Fake tests)</a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;/posts/pragmatic-testing-part-1&quot;&gt;In the last post, we discussed the reasons for writing tests, principles and how to write code that is easy to test&lt;/a&gt;. In this post, we&amp;#39;ll discuss more on how to write good tests.&lt;/p&gt;&lt;p&gt;First, let me repeat the important thing: &lt;strong&gt;Test is a not a silver bullet.&lt;/strong&gt; Adding a few tests won&amp;#39;t help us improve the code quality that much. If you haven&amp;#39;t read the &lt;a href=&quot;/posts/pragmatic-testing-part-1&quot;&gt;part 1&lt;/a&gt; yet, it is highly recommended to read it first.&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;Good-tests&quot;&gt;1. Good tests&lt;/h2&gt;&lt;p&gt;We are finally here! Let&amp;#39;s talk about writing tests! Making code easy to test is only the first step, and to write good tests we will need some more skills.&lt;/p&gt;&lt;h3 id=&quot;Choose-the-right-way-to-test&quot;&gt;1.1. Choose the right way to test&lt;/h3&gt;&lt;p&gt;We know that there are various types of tests. And here are some examples:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Unit test&lt;/strong&gt;: Test a class or a small module by talking to them directly, e.g. calling public member functions.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Module/Service test&lt;/strong&gt;: Test a module or service by simulating its upstream and downstream services.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;End-To-End test (E2E test)&lt;/strong&gt;: Test a complete system by simulating the upstream and downstream services of the whole system.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;Integration test&lt;/strong&gt;: Integrate all services, even multiple systems, as required and test them.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;In-Service/Probe test&lt;/strong&gt;: Test the services within the service itself or with a dedicated probe service at regular intervals.&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
    
      <category term="01 Binary Life" scheme="http://en.r12f.com/categories/01-Binary-Life/"/>
    
    
      <category term="Testing" scheme="http://en.r12f.com/tags/Testing/"/>
    
      <category term="Coding" scheme="http://en.r12f.com/tags/Coding/"/>
    
      <category term="Software Engineering" scheme="http://en.r12f.com/tags/Software-Engineering/"/>
    
  </entry>
  
  <entry>
    <title>Pragmatic testing (P1: Problems, principles and test-friendly code)</title>
    <link href="http://en.r12f.com/posts/pragmatic-testing-part-1/"/>
    <id>http://en.r12f.com/posts/pragmatic-testing-part-1/</id>
    <published>2021-02-04T02:55:02.000Z</published>
    <updated>2021-02-17T04:15:41.358Z</updated>
    
    <content type="html"><![CDATA[<p><strong>&quot;Please write a UT for this change.&quot;</strong> is one of the most frequent comments I leave in the code review. And I am seeing people tend to write minimum tests just for having something to pass the review.</p><p>Well, I kind of understand writing tests is annoying sometimes, but tests are extremely helpful and there are things could help us writing them too. So let&#39;s talk about test!</p><hr><h2 id="quot-Writing-test-is-so-annoying-quot">1. &quot;Writing test is so annoying&quot;</h2><blockquote><p>&quot;Hey! It compiles! Ship it!&quot;</p></blockquote><p>When talking about writing tests, many people find it annoying for simliar reasons:</p><ul><li>Extra work. Most of time, writing tests is very time consuming. And sometimes, it will take even longer time than development itself.</li><li>The thrill of developing a new feature is usually over after the core logic is done.</li><li>Writing test is a tedious work. And you might not even be able to find a single bug after writing tons of tests.</li><li>We have added a lot of tests for a feature. Then the feature changed, and all the tests have to be rewritten, which greatly slows down the development.</li></ul><p>All these questions and complaints are basically because we don&#39;t understand the real intention of testing, and finally fall into the misunderstanding of writing test for the sake of writing tests. And when we are forced to do something, we are never going to feel it will benefit us, even if it really does.</p><a id="more"></a><hr><h2 id="So-why-testing">2. So, why testing?</h2><p>This is the ultimate question indeed. So, what are these tests for?</p><h3 id="Ensures-no-regression-in-the-future">2.1. Ensures no regression in the future</h3><p>Many people feel tests are like disposable items. We only use them once during code review to show the code is working as expected. On the contrary, there is nothing more cost-effective than tests: a once-investment, a lifetime warranty! <strong>Once the tests are done, they will guarantee our code always works in the way we expect</strong>. This is the one of the biggest reasons for us to write tests.</p><p>Furthermore, if it is done right, the marginal cost will be almost 0!</p><p>Don&#39;t underestimate this property. it&#39;s what allows us to make changes to our code with confidence. It is the cornerstone of any code refactoring (see &quot;Refactor: Improving the Design of Existing Code&quot; Ch. 2), and it is also the prerequisite of any automation (see &quot;Continuous Delivery&quot;). And <strong>safe code refactoring and automation is what we are really trying to achieve</strong>.</p><h3 id="Reveals-behavior-changes">2.2. Reveals behavior changes</h3><p>Of course, we can also apply this property above reversely, that is, when we change the behavior of the code, the test should be able to fully reflect all the changes brought to the system. And these changes should be as clear as possible to help us in self-reviewing and code review.</p><p>We&#39;ll leave the specific methodology here and discuss it in next post.</p><h3 id="Tests-are-enforced-docs">2.3. Tests are enforced docs</h3><blockquote><p>&quot;Programs must be written for people to read, and only incidentally for machines to execute.&quot;<br>-- Harold Abelson and Gerald Jay Sussman, from &quot;The Structure and Interpretation of Computer Programs&quot;</p></blockquote><p>The biggest problem with documentation is that it&#39;s not enforced, so it starts to become obsolete from the very first second it&#39;s written. This is why the code itself should act as its own documentation. That&#39;s how we can be sure that the documentation will never become obsolete. Tests are code too. And comparing to other code, they are a even better knowledge base.</p><ol><li>First, tests demonstrate how to use the module that is being tested. If you are familiar with Test Driven Development (TDD), you will know that one of the purposes of writing tests is to help us design and improve the interfaces from the user&#39;s perspective. Although we don&#39;t necessarily have to use TDD, the idea is the same. Languages like golang do a even better job by directly <a href="https://blog.golang.org/examples" target="_blank" rel="noopener">distinguishing and supporting such example tests separately at the language level</a>.</li><li>Second, the assertion in the test guarantees that everything it says will always be true! This gives us a shortcut to understand the code. We don&#39;t have to struggle figuring out what does the code do and understand all the details, we can just read the conclusions that are absolutely true. How good is this! It is the best documentation you can ever imagine!</li></ol><p>For this reason, whenever I need to understand a module, after a brief reading of the design doc, I will usually spend more time reading the relevant tests.</p><hr><h2 id="Principles-of-testing">3. Principles of testing</h2><p>Now that we understand the purpose of testing, you may have a little interest in writing tests. But reality is always stark and the problems we mentioned above are still in front of us, so the methodology of writing tests becomes especially important. Remember: we should never write tests for the sake of writing tests! Instead, we should <strong>test reasonably in reasonable places</strong>. Do not let testing becomes a burden to us, or making us slaves. Make it become our helper!</p><p>To help us test better, we should follow several principles below, no matter if we are writing features or writing tests:</p><ul><li><strong>Shortest distance</strong>: The distance between the error check and the action should be the shortest. In other words, an error should be reported immediately if something goes wrong.</li><li><strong>Observable</strong>: The behavior and state of the program can be easily and clearly observed. For example, logs, error contexts, and other information.</li><li><strong>Repeatable</strong>: The program&#39;s behavior and errors can be stably reproduced. Unstable tests are really annoying.</li></ul><p>We&#39;ll discuss the specific methodology below and explain how to apply these principles.</p><hr><h2 id="Good-code-tests-itself">4. Good code tests itself</h2><p>When many people hear about writing tests, they think of writing test cases one by one in unit tests. In fact, tests don&#39;t necessarily need to be implemented in this way. We know that the earlier we find a problem, the cheaper it is to fix it, so wouldn&#39;t it be more convenient if the code itself could help us find the error?</p><p>So, don&#39;t forget: good code tests itself. This is an application of the shortest distance principle.</p><h3 id="Contract-programming-Design-by-contract">4.1. Contract programming (Design by contract)</h3><p>Although many projects are very complex, they are nothing compared to human society. So what is it that makes our society possible to move forward with such complexity? This is the power of contract! From a small verbal agreement, to a larger contract signed to work, to the ubiquitous rules and regulations, to the laws that guide our lives. They are all contracts. And if we break them, we will be punished in certain way.</p><p>Same can be applied in coding. Each function has its own contract: what are its preconditions (e.g. parameters are not null), what are the postconditions (e.g. a file must be created), and what are the invariants during execution (for example, in binary search, left must never be greater than right). Once the contract is broken, it needs to be punished (return an error or exception). This is <a href="https://en.wikipedia.org/wiki/Design_by_contract" target="_blank" rel="noopener">Design by contract</a>.</p><p>Essentially contract programming is a kind of in-code testing. It is so famous not only because it helps us find the problem, but more importantly it also helps us determine where the problem is. It works as the contracts in real life - you&#39;re late is you&#39;re late. If the preconditions of a function are not met thus causing an error, then we don&#39;t need to check this function at all; if an invariant is violated, then the problem must be in this function or its related functions (e.g., other functions of the same class), and we don&#39;t need to check its caller. This greatly helps us debugging problems!</p><p>In fact, many programming languages already provide support for contract programming. For examples: <a href="https://docs.microsoft.com/en-us/cpp/code-quality/understanding-sal" target="_blank" rel="noopener">SAL</a> (a type of code annotation) and <a href="https://github.com/microsoft/GSL" target="_blank" rel="noopener">GSL</a> (for supporting <a href="https://github.com/isocpp/CppCoreGuidelines" target="_blank" rel="noopener">C++ Core Guidelines</a>) in C/C++, <a href="https://docs.microsoft.com/en-us/dotnet/framework/debug-trace-profile/code-contracts" target="_blank" rel="noopener">CodeContract</a> in C#. And the impact of this idea is so large that if the language doesn&#39;t provide it or the inbox one is slightly inconvenient, the community will help provide third-party packages to implement it, such as <a href="https://github.com/safakgur/guard#introduction" target="_blank" rel="noopener">Dawn.Guard</a> for C# and <a href="https://github.com/go-ozzo/ozzo-validation" target="_blank" rel="noopener">ozzo-validation</a> for Go.</p><h3 id="Be-disciplined-not-permissive">4.2. Be disciplined, not permissive</h3><blockquote><p>&quot;Don&#39;t assume it, prove it.&quot; -- David Thomas, from &quot;The Pragmatic Programmer: Your Journey to Mastery&quot;</p></blockquote><p>According to the shortest distance principle, we want to report errors as early as possible, so a very intuitive conclusion is that - <strong>If there is a more enforceable way to constrain code, always do so</strong>.</p><p>The common constraints, in descending order of strength, are listed as follows:</p><ol><li>Syntax-level compilation errors</li><li>Compile-time assertions</li><li>Static code analysis</li><li>Run-time errors, assertions or exceptions</li><li>Checks in run-time and return error codes, then asserts in tests.</li><li>Checks in run-time, without returning any error codes. But internal state can be accessed later, then asserts in tests.</li><li>No error handling of any kind</li></ol><p>Level 1-3 constraints are compile-time constraints, while level 4-6 constraints are run-time constraints, and they each have their own characteristics. To show how they works, here is an example, which I believe we all have seen a lot:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Data</span> &#123;</span></span><br><span class="line">  <span class="keyword">int</span> FieldA;</span><br><span class="line">  <span class="keyword">char</span> FieldB[<span class="number">6</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">ProcessData</span><span class="params">(Data *p)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (p == <span class="literal">nullptr</span>) &#123; <span class="keyword">return</span> <span class="literal">false</span>; &#125;</span><br><span class="line">  <span class="comment">// Do something here</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Run-time-constraints">4.2.1. Run-time constraints</h4><h5 id="Error-codes">4.2.1.1. Error codes</h5><p>Based on the definitions above, we can tell this function is using level 5 constraint. Since this constraint mainly uses error codes to determine the type of error, it is better to use more expressive error codes. For example, <a href="https://en.cppreference.com/w/cpp/error/error_code" target="_blank" rel="noopener">std::error_code</a>, a strong-typed, extendable, descriptive error code in C++, or integer based error codes likes <a href="https://docs.microsoft.com/en-us/windows/win32/seccrypto/common-hresult-values" target="_blank" rel="noopener">HRESULT</a> or <a href="https://docs.microsoft.com/en-us/windows/win32/debug/system-error-codes" target="_blank" rel="noopener">System Error Code</a> in Windows programming.</p><p>In the example above, using bool is bad, because returning false cannot tell us what might have gone wrong at all. Hence, to improve it, we can replace it with HRESULT:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">HRESULT <span class="title">ProcessData</span><span class="params">(Data *p)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (p == <span class="literal">nullptr</span>) &#123; <span class="keyword">return</span> E_INVALIDARG; &#125;</span><br><span class="line">  <span class="comment">// Do something here</span></span><br><span class="line">  <span class="keyword">return</span> S_OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>As we can see here, the problem with the return code is very obvious. If no test case tests it, this kind of error will not be found by us quickly, until it goes wrong somewhere else and maybe after days of debugging (missing logs again? :P). So in order to find the problem sooner, we can try raising the constraint level of this code.</p><h5 id="Assertions-and-exceptions">4.2.1.2. Assertions and exceptions</h5><p>We are all familar with assertions and exceptions. They are all very straightforward, no matter if we are using exceptions like <a href="https://docs.microsoft.com/en-us/dotnet/api/system.argumentnullexception" target="_blank" rel="noopener">ArgumentNullException</a> or <a href="http://www.cplusplus.com/reference/cassert/assert/" target="_blank" rel="noopener">Assert</a>. But there is a better way to apply assertions and exceptions. That is the contract programming we have already mentioned above.</p><p>If we don&#39;t use any third-party libraries, we can define some macros ourselves to start with. For example, the following code implements the Requires keyword in contract programming to help us constrain our code.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#<span class="meta">#<span class="meta-keyword">define</span> REQUIRES(cond) assert(cond)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ProcessData</span><span class="params">(Data *p)</span> </span>&#123;</span><br><span class="line">  REQUIRES(p != <span class="literal">nullptr</span>);</span><br><span class="line">  <span class="comment">// Do something here</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>A few things to note here are:</p><ul><li>If the argument here is a user input, we must be careful with assertions. A user input can be anything, there is no contractual restriction on it, so we have to handle it properly. Assertions should only be used when something impossible happens, e.g. internal calls. Otherwise, they can lead to unexpected crashes and affect the user experience. (Classic joke: <a href="https://twitter.com/brenankeller/status/1068615953989087232" target="_blank" rel="noopener">A QA engineer walks into a bar</a>).</li><li>If this null pointer is the only thing that can go wrong in this function, we don&#39;t need to return any error anymore, because it&#39;s already asserted, hence changing the return value type to void.</li></ul><p>And as mentioned above, there are already many mature libraries for contract programming. Applying them wisely can make us much more efficient at writing code and debugging issues.</p><h4 id="Compile-time-constraints">4.2.2. Compile-time constraints</h4><p>The benefit of run-time constraints is that it can handle any data. But its problem is that it requires us to execute this code. Therefore, we must write and run specific tests to trigger them, and it also takes time to debug these errors. These are all overhead for us. So, is there a stronger and more convenient constraint? Gladly, yes, that is compile-time constraint.</p><p>The compile-time constraints, which are all executed during compile and generate compile errors if something goes wrong. The advantage of it is very obvious:</p><ul><li>If the compilation fails, we don&#39;t have any software to release. And this prevent us from releasing bad versions for good.</li><li>Compile errors usually come with very detailed information about the error, like which file and which line of code. So comparing to run-time errors, compile errors require almost no debugging (ahem, except for C++ template deduction errors......).</li></ul><p>But the biggest problem with compile-time constraints is that if the state cannot be determined during compilation, we cannot use them (e.g., user input).</p><p>And here, let&#39;s see how to apply the compile-time constraints.</p><h5 id="Static-code-analysis">4.2.2.1. Static code analysis</h5><p>Static code analysis is a useful tool to help us analyze our code and identify possible problems in it, such as memory being allocated but not released, variables being used before initialized, and so on. The reason it is listed only as level 3 here is that static code analysis is usually optional and people often forget to turn it on. In addition, code analysis takes time, which can make compilation speed significantly slower, so many people also disable it during development and only enable it when merging code, so it is not that mandatory, which causes the problems being found late.</p><p>There are many static code analysis tools, such as <a href="https://docs.microsoft.com/en-us/cpp/code-quality/using-the-cpp-core-guidelines-checkers" target="_blank" rel="noopener">CppCoreCheck</a>, <a href="https://docs.microsoft.com/en-us/dotnet/fundamentals/code-analysis/overview" target="_blank" rel="noopener">NetAnalyzer</a> and so on. Many of these tools also require us to modify the code accordingly, telling it what we expect, for example, <a href="https://docs.microsoft.com/en-us/cpp/code-quality/understanding-sal" target="_blank" rel="noopener">SAL</a>.</p><p>Now, let&#39;s apply the level 3 constraint by adding SAL to the same code:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ProcessData</span><span class="params">(_In_ Data *p)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// REQUIRES(p != nullptr); // This line can be removed, if p is not a third-party input, e.g. user input.</span></span><br><span class="line">  <span class="comment">// Do something here</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>As we can see here, the SAL <code>_In_</code> keyword added before the parameter did not just provide the expectation for code analysis, it is also a contract that tells the caller this function does not accept null pointers. So, based on the contract programming, we can assume that the caller abides by the contract and remove null pointer check. This makes the code simplier and more clear, and also more efficient! (Imagine passing a pointer through a call stack, and the beginning of every function on the stack starts with the null pointer check......)</p><h5 id="Assert-not-just-in-run-time">4.2.2.2. Assert, not just in run-time</h5><p>Many languages also provide compile-time assertions, such as <a href="https://en.cppreference.com/w/cpp/language/static_assert" target="_blank" rel="noopener"><code>static_assert</code></a> in C++. This assertion can be used as long as the condition can be evaluated at compile time.</p><p>For example, we can make the following assertion on the above code: (Will both assertions succeed? :D)</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Data</span> &#123;</span></span><br><span class="line">  <span class="keyword">int</span> FieldA;</span><br><span class="line">  <span class="keyword">char</span> FieldB[<span class="number">6</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Making sure structure and fields are aligned when dealing with binary buffers.</span></span><br><span class="line"><span class="keyword">static_assert</span>(<span class="keyword">sizeof</span>(Data) == <span class="number">10</span>);</span><br><span class="line"><span class="keyword">static_assert</span>(FIELD_OFFSET(Data, FieldB) == <span class="number">4</span>);</span><br></pre></td></tr></table></figure><p>In addition to this, there are some other common usage patterns, such as:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Making sure every enum value will have its name.</span></span><br><span class="line"><span class="keyword">static_assert</span>(_countof(enum_names) == enum_value_max));</span><br></pre></td></tr></table></figure><h5 id="Syntax-level-constraints-and-immutability">4.2.2.3. Syntax-level constraints and immutability</h5><p>At last, the highest level of constraint - syntax errors. By using the keywords provided by the language wisely, we can make the language itself help us avoid bugs.</p><p>For example, for fixing the code above, we can simply use reference instead of pointer to avoid the null pointer problem for good. Then, we don&#39;t need any null pointer check at all. Also since it is an <a href="https://docs.microsoft.com/en-us/cpp/code-quality/understanding-sal?#sal-basics" target="_blank" rel="noopener"><code>_In_</code></a> parameter, we can add a <a href="https://en.cppreference.com/w/cpp/language/cv" target="_blank" rel="noopener"><code>const</code> qualifier</a> to ensure it will not be modified by the function.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ProcessData</span><span class="params">(_In_ <span class="keyword">const</span> Data &amp;p)</span> </span>&#123; <span class="comment">/* Do something here */</span> &#125;</span><br></pre></td></tr></table></figure><p>There are many other keywords/syntaxes like these to help us with constraints, such as the frequently used <a href="https://en.cppreference.com/w/cpp/language/final" target="_blank" rel="noopener"><code>final</code> keyword</a>, the <a href="https://en.cppreference.com/w/cpp/language/override" target="_blank" rel="noopener"><code>override</code> keyword</a> in c++, and so on. And my favorite one - <code>const</code>.</p><p>In C++, <a href="https://en.cppreference.com/w/cpp/language/cv" target="_blank" rel="noopener"><code>const</code> qualifier</a> can appear in many places - decorate variables, function parameters, member functions. Whenever I see <code>const</code>, I feel especially relieved, because I know whatever it decorates won&#39;t be changed by anything (or won&#39;t change anything), so it&#39;s safe in multi-threaded programming (there are still exceptions, of course, but they&#39;re much less likely). And this is one of the most important cornerstones of functional programming (or implementing thread-safe programs): <a href="https://en.m.wikipedia.org/wiki/Immutable_object" target="_blank" rel="noopener">Immutability</a>.</p><p>Immutability, for an object, means that the state of the object is immutable after its constructor is done, i.e., the constructor is the only place where the state of the object can be changed. Such an object is also called an immutable object. The affinity of immutable objects for multi-threaded programming is very obvious - it cannot be changed by anyone, so you can access it in parallel or in any way you want.</p><p>Here again, taking the same function as an example. If, say, ProcessData is a callback function and the parameter p will be accessed by multiple threads, then we can implement this function in the following way, so that we can avoid many errors that caused by multi-threaded access, like race conditions.</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">ProcessData</span><span class="params">(_In_ <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;<span class="keyword">const</span> Data&gt; p)</span> </span>&#123; <span class="comment">/* Do something here */</span> &#125;</span><br></pre></td></tr></table></figure><p>Now, immutable objects are not exclusive to functional languages anymore, for example, C# provided <a href="https://docs.microsoft.com/en-us/dotnet/api/system.collections.immutable" target="_blank" rel="noopener">Immutable Collection</a> to help us do better in multi-threaded programming.</p><h3 id="Dead-programs-tell-no-lies">4.3. Dead programs tell no lies</h3><h4 id="Assertions-or-error-codes">4.3.1. Assertions or error codes?</h4><blockquote><p>&quot;Defensive programming is a waste of time. Let it crash!&quot; -- Joe Armstrong, the inventor of Erlang<br>&quot;Crash, don&#39;t trash.&quot; -- David Thomas, from &quot;The Pragmatic Programmer: Your Journey to Mastery&quot;</p></blockquote><p>After the section above, you may be wondering, is it better to use assertions than error codes? If an assertion fails, the program will crash. Shouldn&#39;t this be avoided?</p><p>On the contrary, because crashes are ruthless, we should use them more often. Especially when a major problem is encountered, we should crash as soon as possible instead of remain silent and continuing execution.</p><p>First, crashes never lie! And they also generate a crash dump file (i.e. core dump), which provides the call stack when crash happens, or even complete memory data to tell us why. This is far more informative than one or two lines of error logs, which can greatly increase the observability of a program when something goes wrong.</p><p>Second, when a program is in an uncertain state, it is usually better to just crash than to continue execution, because we cannot predict what the program is going to do next. It may modify user data indiscriminately, it may execute some logic that should not be executed at all, or call another module or service. And the effects of these errors are usually very difficult to recover (Good luck if a user&#39;s data in his file or database is corrupted, due to a bug in our service sending valid request...)</p><p>Finally, crashes just simply can&#39;t be ignored, whether it&#39;s in testing, pre-release, or final release. This gives us two particularly good properties:</p><ul><li>Anything that we asserted is guaranteed to be true. This helps us simplify our code and increases our confidence that our code is executing as expected. Think of a certain assertion in your program that never get triggered when executed on millions of machines.</li><li>Assertion failures are hard to miss. We want any errors to be exposed as early as possible, rather than found by our end user and causing us receiving a bunch of incident tickets.</li></ul><p>Of course, assertions are good, but don&#39;t forget the scenarios, in which we can use them, as already mentioned above. In simple words: <strong>if something is code-wise impossible, it should be asserted, otherwise it should be gracefully handled, such as user input</strong>.</p><h4 id="Suicide-point">4.3.2. Suicide point</h4><blockquote><p>&quot;Let there be light!&quot;</p></blockquote><p>Because crashes have such good properties, we can also use them to better examine the behavior in our program when needed.</p><p>Suicide point is different from assertions. If a program hits them, it dies. But usually, they are not enabled at all. We only turn it on, when we have to.</p><p>I have worked on a service once, which is really old and buggy. In order to understand what the service was really doing, I added a lot of trace points on all key functions in all modules (trace points won&#39;t trigger crashes but only report hit counts), such as whenever core system API call fails. Then after releasing the trace points, I was so surprised to find that the error rate of the core system API calls was so high that all instances combined could reach millions of errors per minute (yes, you are reading it right, millions, no kidding). Some errors are so hard to debug, even with very detailed error logs, because the amount of information was still too limited. So I added suicide support to all the trace points. With this, I could control any instance at will and trigger crash whenever the specific trace point was hit. The crash dumps collected from these suicide points gave me really deep insight on these hard issues with very limited impact, like what does certain memory buffer, that we pass into the system call, looks like. Then, by slowly gathering information and iterating the fixes, the amount of errors in the core system API has now dropped to single digits per hour or even per day. Finally, we also added alerts to these trace points to help us guard any future regressions.</p><hr><h2 id="Good-code-makes-good-tests">5. Good code makes good tests</h2><p>Before writing tests, we should never ignore the quality of the code itself. If the code itself stinks, the tests will be a mess too.</p><p>Please don&#39;t hope how much testing can help us improve the our code quality. No matter how bad the code is, we can still write some test for it, but these tests will suck as well!</p><h3 id="Pragmatic-testing-code-boundary-and-code-refactor">5.1. Pragmatic testing: code boundary and code refactor</h3><p>Many companies or projects enforces all classes and functions must have test without even thinking about it, in order to achieve &quot;absolute 100%&quot; code coverage. Personally, I hate this, because it conflicts with one of the major goal we want to achieve with tests -- helps us refactor the code. Writing code is like planning a city. As long as our project is evolving, refactoring is inevitable. And refactoring inevitably involves changing the code structure within certain boundaries. So this kind of brute force testing will only waste us a lot of time and slow down the development progress, while provides almost zero benefit.</p><p>Therefore, while we want make sure every case is tested, we also need to leave the code certain space to move. A better way to write test is to first observe where the code boundary is, such as submodules or code layers, and then add tests at these boundaries. Most of the code refactors that we do are within these boundaries or rarely cross them, so the tests, that is added this way, will require very little or even no modification at all during refactoring!</p><p>And this brings a change in the code refactoring process, which is the practice I recommend to everyone in our current project:</p><ol><li>Before refactoring any code, add tests and submit them to harden the behavior of the code we want to refactor. If any bugs are found, fix the bugs first.</li><li>Start refactoring the code, with small commits (see &quot;Refactor: Improving the Design of Existing Code&quot; for how to refactor the code in small step). And the principle is: <strong>The code refactor commit should not show any behavior changes to existing tests</strong>. If a test needs to be added, return to the step 1 to add the test first. Then continue refactoring in the step 2.</li></ol><p>The benefits of this are, first, the current behavior of the system will be thoroughly understood before any changes are made; second, when reviewing the code change, it is crystal clear even with just a glance, that I changed the code, but <strong>there is no behavior change at all</strong> to the system, hence it is a safe commit.</p><h3 id="Increase-testable-surface-high-cohesion-low-coupling">5.2. Increase testable surface: high cohesion, low coupling</h3><p>&quot;High cohesion, low coupling&quot;, you may have heard this countless of times. But I still like to mention it here, because it is really helpful for writing tests.</p><p>Highly cohesive and low-coupling code has some characteristics: good modularity, single functionality (<a href="https://zh.wikipedia.org/wiki/%E5%8D%95%E4%B8%80%E5%8A%9F%E8%83%BD%E5%8E%9F%E5%88%99" target="_blank" rel="noopener">Single Responsibility Principle (SRP)</a>) and orthogonal for each part. Such code is very test-friendly. So, if the code you are maintaining is messy, you could consider refactoring it first to improve cohesion and reduce coupling. This can lead to better and more fine-grained module partitioning or code layering, thus providing us with more testable surface (refer to the previous item: &quot;Pragmatic testing: code boundary and code refactor&quot;).</p><p>Another service I worked on was a typical victim of violation of the single responsibility principle. What this module do was actually simple: it is a multi-threaded, multi-protocol probe service. It sends request to other services and see if they are alive or not. The problem with this services is that, the underlying API usage, multi-protocol implementation, probe result handling and concurrency management were all implemented in the same place (well, with a few class inheritance, which is still bad for testing). It caused 2 major problems in the existing tests:</p><ol><li>First, we have no way to do synchronization in testing. This results in 30 seconds sleeps everywhere in the tests and makes the whole test suite takes more than 20 minutes to run.</li><li>Second, there is no way to simulate each error cases and thoroughly test the probe result handling, especially when underlying api return errors in the multi-threaded context.</li></ol><p>In the end, people gave up the unit test and created a test server to do tricky manual test every time. And even so, it only covers the simplest case. This makes this service almost impossible to maintain, and led to a lot of tickets coming to our team.</p><p>So, in order to improve this service, I had to be patient and this is what I did:</p><ol><li>Harden the code behavior by adding and improving the tests first, even through the results are wrong. This step is critical, because it gives us a solid baseline to make sure no regression will be introduced. So, although it is crazily painful, as the tests took forever to run, this has to be done first.</li><li>Improve the logging and metrics data, which improves the observability, especially in scale.</li><li>Refactor the code in small steps. Change the <a href="https://en.m.wikipedia.org/wiki/Composition_over_inheritance" target="_blank" rel="noopener">inheritance to composition</a> to separate api, multi-protocol implementation and concurrency management, which creates new code boundaries. And of course, no behavior change should show up in existing tests. But for each new layer, I will add more tests for it.</li><li>Improve the tests by using this new boundary, but still no behavior change should show up in tests.</li><li>Fix the issues that I found during this process. Each fix will show up as several test failures, which helps me to confirm the the fix.</li></ol><p>Thus, in two weeks, I found and fixed countless problems - incorrect error handling and multithreading issues all over the place. In the end, besides the code quality improvements and bug fixes, not only the number of tests increase significantly, but the time to run all the tests also dropped from 20 minutes to 3 minutes. And the best part is that all intermediate versions could be released or rolled back at any time without worrying about regressions.</p><p>After this, the ticket for complaining this service has greatly reduced. Except for one known &quot;bug&quot; that was a behavior change and could not be fixed directly, the service has gone from a problem-prone area to almost a problem-free one. The new logs and metrics data have also become one of the most important metrics for evaluating build quality.</p><h3 id="Avoid-unstable-code">5.3. Avoid unstable code</h3><p>As mentioned above, one of the principles of writing tests is repeatable: don&#39;t report errors indiscriminately when nothing goes wrong, but be able to reproduce steadily when something goes wrong. If a test always fails two or three times out of ten runs, soon no one will take the failure seriously, because who knows if the error is &quot;expected&quot; or not, and it will pass after maybe two more tries anyway. This situation is a big no-no in testing.</p><p>Most of instability issues are caused by the code itself, that we are testing, being unstable.</p><ul><li>Time sensitive logic or tests. E.g.: timer, sleep, multi-threaded contention, etc.</li><li>Usage of random APIs, e.g.: jitter, random numbers, etc.</li></ul><p>In all these cases, we can first separate out the unstable part, abstract it into a trigger, and then test the trigger and handling logic separately. This way we can easily use various synchronization mechanisms to stabilize our code and tests, such as event or lock.</p><h3 id="Be-friendly-to-tests-test-stub">5.4. Be friendly to tests: test stub</h3><p>I believe we all have encountered more or less the following awkward situations when writing tests.</p><ul><li>Our code needs to call a system API, but this API does not work in the test environment.</li><li>The state of a member variable is really important, and we want to test it to make sure it works, but we don&#39;t want to expose it to our users.</li><li>We need to test the behavior of a class under certain condition or state, but this state or condition is not easy to achieve</li><li>and so on...</li></ul><p>This is where test stub can help.</p><ul><li>A thin layer of wrapping around the system API, allowing us to easily simulate success and failure situations</li><li>If it is our own code, then check if we can create more layers in the code. Once more layers are created, they can be abstracted stably and used as our test stub to help us mock their state.</li><li>The much-maligned friend class in C++ can be a good solution to the problem of checking private member variables: <code>friend class FooTests;</code></li></ul><hr><h2 id="Take-a-break">6. Take a break</h2><p>All right! That&#39;s the end of part 1. If you&#39;ve read this far, you&#39;ll notice that we haven&#39;t even started talking about how to write tests! Yes, and to restate my point here: <strong>Writing tests is not just about writing tests, it&#39;s not our purpose. Test is not a silver bullet, and adding a few tests won&#39;t help us improve the quality of our code much. And good code quality is our actual goal</strong>.</p><p>Ok! Let&#39;s summarize all the things we discussed so far:</p><p>First, we summarized the reason of testing: to ensure the code does not regress; to help us identify changes to the code; and to act as an enforced document and knowledge base.</p><p>Then, we discussed the principles of testing: shortest distance, observable, repeatable.</p><p>Finally, we discussed part of the methodology. In this post, we focused on the code being tested itself:</p><ol><li>Good code tests itself: use contract programming (design by contract); constrain the code behavior as much as possible; use crashes wisely to increase the observability of our program.</li><li>Good code makes good tests: use code boundary to make tests better support refactoring; reduce coupling and improve cohesion to create more testable surfaces; avoid unstable code; use test stub to help us test more deeply.</li></ol><p>In the next post, we&#39;ll discuss how to write good tests.</p><hr><div class="post-series"><div class="post-series-title">Posts in the same series：</div><ul class="post-series-list"><li class="post-series-list-item"><a href="http://en.r12f.com/posts/pragmatic-testing-part-2/">Pragmatic testing (P2: Good tests, Metadata-based testing, Fake tests)</a></li><li class="post-series-list-item"><a href="http://en.r12f.com/posts/pragmatic-testing-part-1/">Pragmatic testing (P1: Problems, principles and test-friendly code)</a></li></ul></div><b>Post link：</b><a href="http://en.r12f.com/posts/pragmatic-testing-part-1/" target="_blank">Pragmatic testing (P1: Problems, principles and test-friendly code)</a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&amp;quot;Please write a UT for this change.&amp;quot;&lt;/strong&gt; is one of the most frequent comments I leave in the code review. And I am seeing people tend to write minimum tests just for having something to pass the review.&lt;/p&gt;&lt;p&gt;Well, I kind of understand writing tests is annoying sometimes, but tests are extremely helpful and there are things could help us writing them too. So let&amp;#39;s talk about test!&lt;/p&gt;&lt;hr&gt;&lt;h2 id=&quot;quot-Writing-test-is-so-annoying-quot&quot;&gt;1. &amp;quot;Writing test is so annoying&amp;quot;&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;&amp;quot;Hey! It compiles! Ship it!&amp;quot;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;When talking about writing tests, many people find it annoying for simliar reasons:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Extra work. Most of time, writing tests is very time consuming. And sometimes, it will take even longer time than development itself.&lt;/li&gt;&lt;li&gt;The thrill of developing a new feature is usually over after the core logic is done.&lt;/li&gt;&lt;li&gt;Writing test is a tedious work. And you might not even be able to find a single bug after writing tons of tests.&lt;/li&gt;&lt;li&gt;We have added a lot of tests for a feature. Then the feature changed, and all the tests have to be rewritten, which greatly slows down the development.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;All these questions and complaints are basically because we don&amp;#39;t understand the real intention of testing, and finally fall into the misunderstanding of writing test for the sake of writing tests. And when we are forced to do something, we are never going to feel it will benefit us, even if it really does.&lt;/p&gt;
    
    </summary>
    
    
      <category term="01 Binary Life" scheme="http://en.r12f.com/categories/01-Binary-Life/"/>
    
    
      <category term="Testing" scheme="http://en.r12f.com/tags/Testing/"/>
    
      <category term="Coding" scheme="http://en.r12f.com/tags/Coding/"/>
    
      <category term="Software Engineering" scheme="http://en.r12f.com/tags/Software-Engineering/"/>
    
  </entry>
  
</feed>
